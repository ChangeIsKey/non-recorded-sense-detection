{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation\n",
    "This notebook performs a round of cross validation of the model on the training data.\n",
    "As training data, the filtered and randomized fivefold split data created by `sort_training_data.ipynb` and `generate_gold_splits` is used.\n",
    "In addition to detailed output in markdown format, precision-recall curves and precision-recall-vs-threshold curves are generated for each fold.\n",
    "\n",
    "### Usage\n",
    "To set the model to be evaluated, the `model_name` variable has to be adjusted in the third cell.\n",
    "\n",
    "All further parameters are set below, where the according description can be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "from random import randrange \n",
    "import scipy\n",
    "from sklearn.metrics import cohen_kappa_score, fbeta_score, accuracy_score, precision_score, recall_score, precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random seed\n",
    "seed = randrange(100000)\n",
    "print(f\"seed: {seed}\")\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usage_embeddings_split = \"../cross_validation/usage_embeddings_split.json\"\n",
    "model_name = \"gloss[3]\"\n",
    "#dictionary_embeddings_file = \"../embeddings/scripts/xl-lexeme/examples[6]_3embeddings.json\"\n",
    "\n",
    "#dictionary_embeddings_file = f\"../../../embeddings/scripts/xl-lexeme/swedish/{model_name}_embeddings.json\"\n",
    "dictionary_embeddings_file = f\"../data/outputs/sense_embeddings/english/{model_name}_embeddings.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spearmanr(x, y):\n",
    "    return scipy.stats.spearmanr(x, y)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dictionary_embeddings_file) as f:\n",
    "    dictionary_embeddings = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarities(split_file, sim_measure):\n",
    "    with open(split_file) as f:\n",
    "        usage_embeddings = json.load(f)\n",
    "            \n",
    "        # compare sense embeddings with usage embeddings\n",
    "        unknown_senses = usage_embeddings[\"unknown_senses\"]\n",
    "        #print(unknown_senses)\n",
    "        results = []\n",
    "        for usage in usage_embeddings[\"data\"]:\n",
    "            lemma = usage[\"lemma\"] # task data\n",
    "            usage_embedding = usage[\"embedding\"] # task data\n",
    "\n",
    "            assigned = usage[\"assigned\"] # gold data\n",
    "\n",
    "            if lemma not in dictionary_embeddings.keys():\n",
    "                print(f\"lemma {lemma} not found in dictionary\")\n",
    "                continue\n",
    "            dictionary_entry = dictionary_embeddings[lemma] # dictionary entry of lemma\n",
    "\n",
    "            sense_ranking = []\n",
    "            \n",
    "            # collect all senses that have usages\n",
    "            sense_inventory = []\n",
    "            for sense in dictionary_entry:\n",
    "                if sense[\"usages\"] !=[] and sense[\"sense\"] not in unknown_senses: # filter out senses without usages\n",
    "                    sense_inventory.append(sense)\n",
    "\n",
    "                    \n",
    "            if sense_inventory == []:\n",
    "                senses = [sense[\"sense\"] for sense in dictionary_entry if sense[\"usages\"] != []]\n",
    "                print(f\"empty sense inventory for {lemma}\")\n",
    "            else:\n",
    "                sense_embedding = sense_inventory[0][\"embedding\"]\n",
    "                if sim_measure == \"cosine\":\n",
    "                    most_frequent_similarity = cosine_similarity(usage_embedding, sense_embedding)    \n",
    "                elif sim_measure == \"spearmanr\":\n",
    "                    most_frequent_similarity = spearmanr(usage_embedding, sense_embedding)\n",
    "\n",
    "            for sense in sense_inventory:\n",
    "                sense_embedding = sense[\"embedding\"]\n",
    "\n",
    "                # calculate similarity\n",
    "                if sim_measure == \"cosine\":\n",
    "                    similarity = cosine_similarity(usage_embedding, sense_embedding)\n",
    "                elif sim_measure == \"spearmanr\":\n",
    "                    similarity = spearmanr(usage_embedding, sense_embedding)\n",
    "                \n",
    "                sense_ranking.append(similarity)\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "            sense_ranking = sorted(sense_ranking, reverse=True)\n",
    "\n",
    "            results.append({\n",
    "                \"lemma\": lemma,\n",
    "                \"fold\": usage[\"fold\"],\n",
    "                \"assigned\": assigned,\n",
    "                \"sense_ranking\": sense_ranking,\n",
    "                \"most_frequent_sense\": most_frequent_similarity,\n",
    "            })\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compares \n",
    "def evaluate(threshold, splits, results):\n",
    "    gold_annotations = []\n",
    "    model_annotations = []\n",
    "    random_annotations = []\n",
    "    frequency_annotations = []\n",
    "    minority_annotations = []\n",
    "    majority_annotations = []\n",
    "\n",
    "    for res in results:\n",
    "        if res[\"fold\"] in splits: # only evaluate on the splits we want\n",
    "            unassigned = not res[\"assigned\"] # gold data\n",
    "            sense_ranking = res[\"sense_ranking\"] # task data\n",
    "\n",
    "            if len(sense_ranking) == 0: # no senses in dictionary\n",
    "                print(f\"no senses for {res['lemma']}\")\n",
    "                continue\n",
    "            \n",
    "            gold_annotations.append(int(unassigned)) # append 1 if unassigned, 0 if assigned\n",
    "            \n",
    "            top_sense_similarity = sense_ranking[0] # similarity of top sense\n",
    "\n",
    "            ratio = gold_annotations.count(0) / len(gold_annotations) # ratio of assigned senses\n",
    "\n",
    "            model_annotations.append(int(top_sense_similarity < threshold)) # append 1 if unassigned 0 if assigned\n",
    "            random_annotations.append(int(random.random() > ratio)) # append 1 if unassigned, 0 if assigned\n",
    "            frequency_annotations.append(int(res[\"most_frequent_sense\"] < threshold))\n",
    "            minority_annotations.append(1) # always predict assigned, because majority is assigned\n",
    "            majority_annotations.append(int(random.random() > 0.9)) # predict assigned with 90% probability\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"splits\": splits,\n",
    "        \"threshold\": threshold,\n",
    "        \"gold_annotations\": gold_annotations,\n",
    "        \"model_annotations\": model_annotations,\n",
    "        \"random_annotations\": random_annotations,\n",
    "        \"frequency_annotations\": frequency_annotations,\n",
    "        \"minority_annotations\": minority_annotations,\n",
    "        \"majority_annotations\": majority_annotations,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate measurements for a single threshold\n",
    "def calculate_measurements(res):\n",
    "    gold_annotations = res[\"gold_annotations\"]\n",
    "    model_annotations = res[\"model_annotations\"]\n",
    "    random_annotations = res[\"random_annotations\"]\n",
    "    frequency_annotations = res[\"frequency_annotations\"]\n",
    "    minority_annotations = res[\"minority_annotations\"]\n",
    "    majority_annotations = res[\"majority_annotations\"]\n",
    "\n",
    "    # calculate f_beta, krippendorff_alpha, cohens_kappa\n",
    "    f_5 = fbeta_score(gold_annotations, model_annotations, average=\"binary\", beta=0.5)\n",
    "    f_3 = fbeta_score(gold_annotations, model_annotations, average=\"binary\", beta=0.3)\n",
    "    f_1 = fbeta_score(gold_annotations, model_annotations, average=\"binary\", beta=0.1)\n",
    "    \n",
    "    # calulate random \n",
    "    random_f_5 = fbeta_score(gold_annotations, random_annotations, average=\"binary\", beta=0.5)\n",
    "    random_f_3 = fbeta_score(gold_annotations, random_annotations, average=\"binary\", beta=0.3)\n",
    "    random_f_1 = fbeta_score(gold_annotations, random_annotations, average=\"binary\", beta=0.1)\n",
    "    \n",
    "    # calulate frequency\n",
    "    frequency_f_5 = fbeta_score(gold_annotations, frequency_annotations, average=\"binary\", beta=0.5)\n",
    "    frequency_f_3 = fbeta_score(gold_annotations, frequency_annotations, average=\"binary\", beta=0.3)\n",
    "    frequency_f_1 = fbeta_score(gold_annotations, frequency_annotations, average=\"binary\", beta=0.1)\n",
    "    \n",
    "    # calulate minority\n",
    "    minority_f_5 = fbeta_score(gold_annotations, minority_annotations, average=\"binary\", beta=0.5)\n",
    "    minority_f_3 = fbeta_score(gold_annotations, minority_annotations, average=\"binary\", beta=0.3)\n",
    "    minority_f_1 = fbeta_score(gold_annotations, minority_annotations, average=\"binary\", beta=0.1)\n",
    "    \n",
    "    # calulate majority\n",
    "    majority_f_5 = fbeta_score(gold_annotations, majority_annotations, average=\"binary\", beta=0.5)\n",
    "    majority_f_3 = fbeta_score(gold_annotations, majority_annotations, average=\"binary\", beta=0.3)\n",
    "    majority_f_1 = fbeta_score(gold_annotations, majority_annotations, average=\"binary\", beta=0.1)\n",
    "    \n",
    "    # calculate accuracy, precision, recall\n",
    "    accuracy = accuracy_score(gold_annotations, model_annotations)\n",
    "    precision = precision_score(gold_annotations, model_annotations, zero_division=0)\n",
    "    recall = recall_score(gold_annotations, model_annotations, zero_division=0)\n",
    "\n",
    "    tp = sum([1 for i in range(len(gold_annotations)) if gold_annotations[i] == 1 and model_annotations[i] == 1])\n",
    "    fp = sum([1 for i in range(len(gold_annotations)) if gold_annotations[i] == 0 and model_annotations[i] == 1])\n",
    "    fn = sum([1 for i in range(len(gold_annotations)) if gold_annotations[i] == 1 and model_annotations[i] == 0])\n",
    "    tn = sum([1 for i in range(len(gold_annotations)) if gold_annotations[i] == 0 and model_annotations[i] == 0])\n",
    "\n",
    "    return {\n",
    "        \"split_label\": res[\"splits\"],\n",
    "        \"threshold\": res[\"threshold\"],\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f_5\": f_5,\n",
    "        \"f_3\": f_3,\n",
    "        \"f_1\": f_1,\n",
    "        \"tp\": tp,\n",
    "        \"fp\": fp,\n",
    "        \"fn\": fn,\n",
    "        \"tn\": tn,\n",
    "        \"random_f_5\": random_f_5,\n",
    "        \"random_f_3\": random_f_3,\n",
    "        \"random_f_1\": random_f_1,        \n",
    "        \"frequency_f_5\": frequency_f_5,\n",
    "        \"frequency_f_3\": frequency_f_3,\n",
    "        \"frequency_f_1\": frequency_f_1,\n",
    "        \"minority_f_5\": minority_f_5,\n",
    "        \"minority_f_3\": minority_f_3,\n",
    "        \"minority_f_1\": minority_f_1,\n",
    "        \"majority_f_5\": majority_f_5,\n",
    "        \"majority_f_3\": majority_f_3,\n",
    "        \"majority_f_1\": majority_f_1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(test_splits, results):\n",
    "    train_results = []\n",
    "    train_splits = [s for s in range(5) if s not in test_splits] # all splits except the one we want to test on\n",
    "\n",
    "    for t in range(0, 100, 1): # test all thresholds from 0 to 1\n",
    "        threshold = t/100\n",
    "        out = evaluate(threshold, train_splits, results) # evaluate on all splits except the one we want to test on \n",
    "        out[\"split\"] = test_splits # add split label\n",
    "        train_results.append(calculate_measurements(out)) # calculate measurements for each threshold\n",
    "\n",
    "    # sort results by f, krippendorff_alpha, cohens_kappa\n",
    "    train_results_f_5 = sorted(train_results, key=lambda x: x[\"f_5\"], reverse=True) \n",
    "    train_results_f_3 = sorted(train_results, key=lambda x: x[\"f_3\"], reverse=True)\n",
    "    train_results_f_1 = sorted(train_results, key=lambda x: x[\"f_1\"], reverse=True)\n",
    "\n",
    "    # use best threshold of each measurement to evaluate on test data\n",
    "    test_f_5 = calculate_measurements(evaluate(train_results_f_5[0][\"threshold\"], test_splits, results))\n",
    "    test_f_3 = calculate_measurements(evaluate(train_results_f_3[0][\"threshold\"], test_splits, results))\n",
    "    test_f_1 = calculate_measurements(evaluate(train_results_f_1[0][\"threshold\"], test_splits, results))\n",
    "\n",
    "    # random baseline\n",
    "    random_results_f_5 = sorted(train_results, key=lambda x: x[\"random_f_5\"], reverse=True)\n",
    "    random_results_f_3 = sorted(train_results, key=lambda x: x[\"random_f_3\"], reverse=True)\n",
    "    random_results_f_1 = sorted(train_results, key=lambda x: x[\"random_f_1\"], reverse=True)\n",
    "\n",
    "    # random baseline test\n",
    "    test_f_5_random = calculate_measurements(evaluate(random_results_f_5[0][\"threshold\"], test_splits, results))\n",
    "    test_f_3_random = calculate_measurements(evaluate(random_results_f_3[0][\"threshold\"], test_splits, results))\n",
    "    test_f_1_random = calculate_measurements(evaluate(random_results_f_1[0][\"threshold\"], test_splits, results))\n",
    "\n",
    "    # frequency baseline\n",
    "    frequency_results_f_5 = sorted(train_results, key=lambda x: x[\"frequency_f_5\"], reverse=True)\n",
    "    frequency_results_f_3 = sorted(train_results, key=lambda x: x[\"frequency_f_3\"], reverse=True)\n",
    "    frequency_results_f_1 = sorted(train_results, key=lambda x: x[\"frequency_f_1\"], reverse=True)\n",
    "    \n",
    "    # frequency baseline test\n",
    "    test_f_5_frequency = calculate_measurements(evaluate(frequency_results_f_5[0][\"threshold\"], test_splits, results))\n",
    "    test_f_3_frequency = calculate_measurements(evaluate(frequency_results_f_3[0][\"threshold\"], test_splits, results))\n",
    "    test_f_1_frequency = calculate_measurements(evaluate(frequency_results_f_1[0][\"threshold\"], test_splits, results))\n",
    "\n",
    "    # minority baseline\n",
    "    minority_results_f_5 = sorted(train_results, key=lambda x: x[\"minority_f_5\"], reverse=True)\n",
    "    minority_results_f_3 = sorted(train_results, key=lambda x: x[\"minority_f_3\"], reverse=True)\n",
    "    minority_results_f_1 = sorted(train_results, key=lambda x: x[\"minority_f_1\"], reverse=True)\n",
    "    \n",
    "    # minority baseline test\n",
    "    test_f_5_minority = calculate_measurements(evaluate(minority_results_f_5[0][\"threshold\"], test_splits, results))\n",
    "    test_f_3_minority = calculate_measurements(evaluate(minority_results_f_3[0][\"threshold\"], test_splits, results))\n",
    "    test_f_1_minority = calculate_measurements(evaluate(minority_results_f_1[0][\"threshold\"], test_splits, results))\n",
    "\n",
    "    # majority baseline\n",
    "    majority_results_f_5 = sorted(train_results, key=lambda x: x[\"majority_f_5\"], reverse=True)\n",
    "    majority_results_f_3 = sorted(train_results, key=lambda x: x[\"majority_f_3\"], reverse=True)\n",
    "    majority_results_f_1 = sorted(train_results, key=lambda x: x[\"majority_f_1\"], reverse=True)\n",
    "\n",
    "    # majority baseline test\n",
    "    test_f_5_majority = calculate_measurements(evaluate(majority_results_f_5[0][\"threshold\"], test_splits, results))\n",
    "    test_f_3_majority = calculate_measurements(evaluate(majority_results_f_3[0][\"threshold\"], test_splits, results))\n",
    "    test_f_1_majority = calculate_measurements(evaluate(majority_results_f_1[0][\"threshold\"], test_splits, results))\n",
    "\n",
    "    return {\n",
    "        \"train_results\": train_results,\n",
    "        \"f_5\": {\n",
    "            \"training\": train_results_f_5,\n",
    "            \"test\": test_f_5\n",
    "        },\n",
    "        \"f_3\": {\n",
    "            \"training\": train_results_f_3,\n",
    "            \"test\": test_f_3\n",
    "        },\n",
    "        \"f_1\": {\n",
    "            \"training\": train_results_f_1,\n",
    "            \"test\": test_f_1\n",
    "        },\n",
    "        \"random_f_5\": {\n",
    "            \"training\": random_results_f_5,\n",
    "            \"test\": test_f_5_random\n",
    "        },\n",
    "        \"random_f_3\": {\n",
    "            \"training\": random_results_f_3,\n",
    "            \"test\": test_f_3_random\n",
    "        },\n",
    "        \"random_f_1\": {\n",
    "            \"training\": random_results_f_1,\n",
    "            \"test\": test_f_1_random\n",
    "        },\n",
    "        \"frequency_f_5\": {\n",
    "            \"training\": frequency_results_f_5,\n",
    "            \"test\": test_f_5_frequency\n",
    "        },\n",
    "        \"frequency_f_3\": {\n",
    "            \"training\": frequency_results_f_3,\n",
    "            \"test\": test_f_3_frequency\n",
    "        },\n",
    "        \"frequency_f_1\": {\n",
    "            \"training\": frequency_results_f_1,\n",
    "            \"test\": test_f_1_frequency\n",
    "        },\n",
    "        \"minority_f_5\": {\n",
    "            \"training\": minority_results_f_5,\n",
    "            \"test\": test_f_5_minority\n",
    "        },\n",
    "        \"minority_f_3\": {\n",
    "            \"training\": minority_results_f_3,\n",
    "            \"test\": test_f_3_minority\n",
    "        },\n",
    "        \"minority_f_1\": {\n",
    "            \"training\": minority_results_f_1,\n",
    "            \"test\": test_f_1_minority\n",
    "        },\n",
    "        \"majority_f_5\": {\n",
    "            \"training\": majority_results_f_5,\n",
    "            \"test\": test_f_5_majority\n",
    "        },\n",
    "        \"majority_f_3\": {\n",
    "            \"training\": majority_results_f_3,\n",
    "            \"test\": test_f_3_majority\n",
    "        },\n",
    "        \"majority_f_1\": {\n",
    "            \"training\": majority_results_f_1,\n",
    "            \"test\": test_f_1_majority\n",
    "        }\n",
    "    }\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(cv_results):\n",
    "    save = []\n",
    "    for measure in [\"f_5\", \"f_3\", \"f_1\"]:\n",
    "        average_threshold = np.mean([cv[measure]['test']['threshold'] for cv in cv_results])\n",
    "        median_threshold = np.median([cv[measure]['test']['threshold'] for cv in cv_results])\n",
    "        threshold_deviation = np.std([cv[measure]['test']['threshold'] for cv in cv_results])\n",
    "        \n",
    "        average_measure_training = np.mean([cv[measure]['training'][0][measure] for cv in cv_results]) # highest score of training\n",
    "        average_measure_test = np.mean([cv[measure]['test'][measure] for cv in cv_results]) # score of test\n",
    "\n",
    "        average_precision_training = np.mean([cv[measure]['training'][0]['precision'] for cv in cv_results])\n",
    "        average_precision_test = np.mean([cv[measure]['test']['precision'] for cv in cv_results])\n",
    "\n",
    "        average_recall_training = np.mean([cv[measure]['training'][0]['recall'] for cv in cv_results])\n",
    "        average_recall_test = np.mean([cv[measure]['test']['recall'] for cv in cv_results])\n",
    "\n",
    "        average_random_training = np.mean([cv[f'random_{measure}']['training'][0][f'random_{measure}'] for cv in cv_results])\n",
    "        average_random_test = np.mean([cv[f'random_{measure}']['test'][f'random_{measure}'] for cv in cv_results])\n",
    "\n",
    "        average_frequency_training = np.mean([cv[f'frequency_{measure}']['training'][0][f'frequency_{measure}'] for cv in cv_results])\n",
    "        average_frequency_test = np.mean([cv[f'frequency_{measure}']['test'][f'frequency_{measure}'] for cv in cv_results])\n",
    "\n",
    "        average_minority_training = np.mean([cv[f'minority_{measure}']['training'][0][f'minority_{measure}'] for cv in cv_results])\n",
    "        average_minority_test = np.mean([cv[f'minority_{measure}']['test'][f'minority_{measure}'] for cv in cv_results])\n",
    "\n",
    "        average_majority_training = np.mean([cv[f'majority_{measure}']['training'][0][f'majority_{measure}'] for cv in cv_results])\n",
    "        average_majority_test = np.mean([cv[f'majority_{measure}']['test'][f'majority_{measure}'] for cv in cv_results])\n",
    "\n",
    "        threshold_splits = [cv[measure]['test']['threshold'] for cv in cv_results]\n",
    "        measure_training_splits = [cv[measure]['training'][0][measure] for cv in cv_results]\n",
    "        measure_test_splits = [cv[measure]['test'][measure] for cv in cv_results]\n",
    "\n",
    "        precision_training_splits = [cv[measure]['training'][0]['precision'] for cv in cv_results]\n",
    "        precision_test_splits = [cv[measure]['test']['precision'] for cv in cv_results]\n",
    "\n",
    "        recall_training_splits = [cv[measure]['training'][0]['recall'] for cv in cv_results]\n",
    "        recall_test_splits = [cv[measure]['test']['recall'] for cv in cv_results]\n",
    "\n",
    "        random_training_splits = [cv[f'random_{measure}']['training'][0][f'random_{measure}'] for cv in cv_results]\n",
    "        random_test_splits = [cv[f'random_{measure}']['test'][f'random_{measure}'] for cv in cv_results]\n",
    "\n",
    "        frequency_training_splits = [cv[f'frequency_{measure}']['training'][0][f'frequency_{measure}'] for cv in cv_results]\n",
    "        frequency_test_splits = [cv[f'frequency_{measure}']['test'][f'frequency_{measure}'] for cv in cv_results]\n",
    "\n",
    "        minority_training_splits = [cv[f'minority_{measure}']['training'][0][f'minority_{measure}'] for cv in cv_results]\n",
    "        minority_test_splits = [cv[f'minority_{measure}']['test'][f'minority_{measure}'] for cv in cv_results]\n",
    "\n",
    "        majority_training_splits = [cv[f'majority_{measure}']['training'][0][f'majority_{measure}'] for cv in cv_results]\n",
    "        majority_test_splits = [cv[f'majority_{measure}']['test'][f'majority_{measure}'] for cv in cv_results]\n",
    "\n",
    "        save.append({\n",
    "            \"measure\": measure,\n",
    "            \"average_threshold\": average_threshold,\n",
    "            \"average_measure_training\": average_measure_training,\n",
    "            \"average_measure_test\": average_measure_test,\n",
    "            \"average_precision_training\": average_precision_training,\n",
    "            \"average_precision_test\": average_precision_test,\n",
    "            \"average_recall_training\": average_recall_training,\n",
    "            \"average_recall_test\": average_recall_test,\n",
    "            \"average_random_training\": average_random_training,\n",
    "            \"average_random_test\": average_random_test,\n",
    "            \"average_frequency_training\": average_frequency_training,\n",
    "            \"average_frequency_test\": average_frequency_test,\n",
    "            \"average_minority_training\": average_minority_training,\n",
    "            \"average_minority_test\": average_minority_test,\n",
    "            \"average_majority_training\": average_majority_training,\n",
    "            \"average_majority_test\": average_majority_test\n",
    "        })\n",
    "\n",
    "        # threshold table\n",
    "\n",
    "        print(f\"\\n#### Threshold statistics\")\n",
    "        print(f\"\\n\")\n",
    "\n",
    "        print(f\"| Measure          | Average  | Median | Deviation |\")\n",
    "        print(f\"| ---------------- | -------- | ------ | --------- |\")\n",
    "        print(f\"| {measure}        | {average_threshold:.3f}      | {median_threshold:.3f}    | {threshold_deviation:.3f}        |\")\n",
    "\n",
    "        print(f\"\\n\")\n",
    "        print(f\"|                  | Average  |      | Fold 1  |      | Fold 2  |      | Fold 3  |      | Fold 4  |      | Fold 5  |      |\")\n",
    "        print(f\"| ---------------- | -------- | ---- | -------- | ---- | -------- | ---- | -------- | ---- | -------- | ---- | -------- | ---- |\")\n",
    "        print(f\"|                  | Training | Test | Training | Test | Training | Test | Training | Test | Training | Test | Training | Test |\")\n",
    "        print(f\"| Threshold        | {average_threshold:.3f}      |      | {threshold_splits[0]:.3f}      |      | {threshold_splits[1]:.3f}      |      | {threshold_splits[2]:.3f}      |      | {threshold_splits[3]:.3f}      |      | {threshold_splits[4]:.3f}      |      |\")\n",
    "        print(f\"| Precision        | {average_precision_training:.3f}      | {average_precision_test:.3f}      | {precision_training_splits[0]:.3f}      | {precision_test_splits[0]:.3f}      | {precision_training_splits[1]:.3f}      | {precision_test_splits[1]:.3f}      | {precision_training_splits[2]:.3f}      | {precision_test_splits[2]:.3f}      | {precision_training_splits[3]:.3f}      | {precision_test_splits[3]:.3f}      | {precision_training_splits[4]:.3f}      | {precision_test_splits[4]:.3f}      |\")\n",
    "        print(f\"| Recall           | {average_recall_training:.3f}      | {average_recall_test:.3f}      | {recall_training_splits[0]:.3f}      | {recall_test_splits[0]:.3f}      | {recall_training_splits[1]:.3f}      | {recall_test_splits[1]:.3f}      | {recall_training_splits[2]:.3f}      | {recall_test_splits[2]:.3f}      | {recall_training_splits[3]:.3f}      | {recall_test_splits[3]:.3f}      | {recall_training_splits[4]:.3f}      | {recall_test_splits[4]:.3f}      |\")\n",
    "        print(f\"| {measure}        | {average_measure_training:.3f}      | {average_measure_test:.3f}      | {measure_training_splits[0]:.3f}      | {measure_test_splits[0]:.3f}      | {measure_training_splits[1]:.3f}      | {measure_test_splits[1]:.3f}      | {measure_training_splits[2]:.3f}      | {measure_test_splits[2]:.3f}      | {measure_training_splits[3]:.3f}      | {measure_test_splits[3]:.3f}      | {measure_training_splits[4]:.3f}      | {measure_test_splits[4]:.3f}      |\")\n",
    "        print(f\"| random_{measure} | {average_random_training:.3f}      | {average_random_test:.3f}      | {random_training_splits[0]:.3f}      | {random_test_splits[0]:.3f}      | {random_training_splits[1]:.3f}      | {random_test_splits[1]:.3f}      | {random_training_splits[2]:.3f}      | {random_test_splits[2]:.3f}      | {random_training_splits[3]:.3f}      | {random_test_splits[3]:.3f}      | {random_training_splits[4]:.3f}      | {random_test_splits[4]:.3f}      |\")\n",
    "        print(f\"| frequency_{measure} | {average_frequency_training:.3f}      | {average_frequency_test:.3f}      | {frequency_training_splits[0]:.3f}      | {frequency_test_splits[0]:.3f}      | {frequency_training_splits[1]:.3f}      | {frequency_test_splits[1]:.3f}      | {frequency_training_splits[2]:.3f}      | {frequency_test_splits[2]:.3f}      | {frequency_training_splits[3]:.3f}      | {frequency_test_splits[3]:.3f}      | {frequency_training_splits[4]:.3f}      | {frequency_test_splits[4]:.3f}      |\")\n",
    "        print(f\"| minority_{measure} | {average_minority_training:.3f}      | {average_minority_test:.3f}      | {minority_training_splits[0]:.3f}      | {minority_test_splits[0]:.3f}      | {minority_training_splits[1]:.3f}      | {minority_test_splits[1]:.3f}      | {minority_training_splits[2]:.3f}      | {minority_test_splits[2]:.3f}      | {minority_training_splits[3]:.3f}      | {minority_test_splits[3]:.3f}      | {minority_training_splits[4]:.3f}      | {minority_test_splits[4]:.3f}      |\")\n",
    "        print(f\"| majority_{measure} | {average_majority_training:.3f}      | {average_majority_test:.3f}      | {majority_training_splits[0]:.3f}      | {majority_test_splits[0]:.3f}      | {majority_training_splits[1]:.3f}      | {majority_test_splits[1]:.3f}      | {majority_training_splits[2]:.3f}      | {majority_test_splits[2]:.3f}      | {majority_training_splits[3]:.3f}      | {majority_test_splits[3]:.3f}      | {majority_training_splits[4]:.3f}      | {majority_test_splits[4]:.3f}      |\")\n",
    "\n",
    "    return save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust further variables below:\n",
    "- `similarity_measure`: Either `cosine` or `spearmanr`\n",
    "- adjust the range of the first for-loop to the number of rounds of cross validation to be performed (note that there need to be enough split files in the according directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = []\n",
    "average_saves = []\n",
    "#similarity_measure = \"spearmanr\"\n",
    "similarity_measure = \"cosine\"\n",
    "for fold_split in range(1, 2):\n",
    "    split_file = f\"../data/outputs/cross_validation/FILTERED_english_assigned_usage_embeddings_{fold_split}.json\"\n",
    "    \n",
    "    results = calculate_similarities(split_file, similarity_measure)\n",
    "    cv_results = []\n",
    "    for split in range(5): # 5-fold cross validation\n",
    "        cv = cross_validate([split], results)\n",
    "        cv_results.append(cv)\n",
    "\n",
    "    print(f\"\\n### Round {fold_split}\")\n",
    "    save = print_results(cv_results)\n",
    "    average_saves.append(save)\n",
    "    \n",
    "    model_results.append({\n",
    "        \"fold_split\": fold_split,\n",
    "        \"measure\": similarity_measure,\n",
    "        \"results\": cv_results\n",
    "    })\n",
    "\n",
    "measures = []\n",
    "f_5_saves = []\n",
    "f_3_saves = []\n",
    "f_1_saves = []\n",
    "\n",
    "for a in average_saves:\n",
    "    f_5_saves.extend([s for s in a if s[\"measure\"] == \"f_5\"])\n",
    "    f_3_saves.extend([s for s in a if s[\"measure\"] == \"f_3\"])\n",
    "    f_1_saves.extend([s for s in a if s[\"measure\"] == \"f_1\"])\n",
    "\n",
    "measures.extend(f_5_saves)\n",
    "measures.extend(f_3_saves)\n",
    "measures.extend(f_1_saves)\n",
    "\n",
    "stats = {}\n",
    "\n",
    "for m in [\"f_5\", \"f_3\", \"f_1\"]:\n",
    "    for measure in measures:\n",
    "\n",
    "        score_deviation = np.std([me[\"average_measure_training\"] for me in measures if me[\"measure\"] == m])\n",
    "    \n",
    "        average_threshold = np.mean([me[\"average_threshold\"] for me in measures if me[\"measure\"] == m])\n",
    "        median_threshold = np.median([me[\"average_threshold\"] for me in measures if me[\"measure\"] == m])\n",
    "        threshold_deviation = np.std([me[\"average_threshold\"] for me in measures if me[\"measure\"] == m])\n",
    "        average_measure_training = np.mean([me[\"average_measure_training\"] for me in measures if me[\"measure\"] == m])\n",
    "        average_measure_test = np.mean([me[\"average_measure_test\"] for me in measures if me[\"measure\"] == m])\n",
    "\n",
    "        average_precision_training = np.mean([me[\"average_precision_training\"] for me in measures if me[\"measure\"] == m])\n",
    "        average_precision_test = np.mean([me[\"average_precision_test\"] for me in measures if me[\"measure\"] == m])\n",
    "\n",
    "        average_recall_training = np.mean([me[\"average_recall_training\"] for me in measures if me[\"measure\"] == m])\n",
    "        average_recall_test = np.mean([me[\"average_recall_test\"] for me in measures if me[\"measure\"] == m])\n",
    "\n",
    "        average_random_training = np.mean([me[\"average_random_training\"] for me in measures if me[\"measure\"] == m])\n",
    "        average_random_test = np.mean([me[\"average_random_test\"] for me in measures if me[\"measure\"] == m])\n",
    "\n",
    "        average_frequency_training = np.mean([me[\"average_frequency_training\"] for me in measures if me[\"measure\"] == m])\n",
    "        average_frequency_test = np.mean([me[\"average_frequency_test\"] for me in measures if me[\"measure\"] == m])\n",
    "\n",
    "        average_minority_training = np.mean([me[\"average_minority_training\"] for me in measures if me[\"measure\"] == m])\n",
    "        average_minority_test = np.mean([me[\"average_minority_test\"] for me in measures if me[\"measure\"] == m])\n",
    "        \n",
    "        average_majority_training = np.mean([me[\"average_majority_training\"] for me in measures if me[\"measure\"] == m])\n",
    "        average_majority_test = np.mean([me[\"average_majority_test\"] for me in measures if me[\"measure\"] == m])\n",
    "\n",
    "        stats[m] = {\n",
    "            \"average_threshold\": average_threshold,\n",
    "            \"median_threshold\": median_threshold,\n",
    "            \"threshold_deviation\": threshold_deviation,\n",
    "            \"average_measure_training\": average_measure_training,\n",
    "            \"average_measure_test\": average_measure_test,\n",
    "            \"average_precision_training\": average_precision_training,\n",
    "            \"average_precision_test\": average_precision_test,\n",
    "            \"average_recall_training\": average_recall_training,\n",
    "            \"average_recall_test\": average_recall_test,\n",
    "            \"average_random_training\": average_random_training,\n",
    "            \"average_random_test\": average_random_test,\n",
    "            \"average_frequency_training\": average_frequency_training,\n",
    "            \"average_frequency_test\": average_frequency_test,\n",
    "            \"average_minority_training\": average_minority_training,\n",
    "            \"average_minority_test\": average_minority_test,\n",
    "            \"average_majority_training\": average_majority_training,\n",
    "            \"average_majority_test\": average_majority_test,\n",
    "            \"score_deviation\": score_deviation\n",
    "        }\n",
    "\n",
    "similarity_labels = {\n",
    "    \"cosine\": \"Cosine-Similarity\",\n",
    "    \"spearmanr\": \"Spearman's Rho\"\n",
    "}\n",
    "\n",
    "measure_labels = {\n",
    "    \"f_5\": \"F-Score (0.5)\",\n",
    "    \"f_3\": \"F-Score (0.3)\",\n",
    "    \"f_1\": \"F-Score (0.1)\",\n",
    "}\n",
    "\n",
    "print(f\"## {similarity_labels[similarity_measure]}\")\n",
    "print(f\"\\n### Threshold statistics\")\n",
    "print(f\"\\n\")\n",
    "print(f\"| Measure          | Average  | Median | Deviation |\")\n",
    "print(f\"| ---------------- | -------- | ------ | --------- |\")\n",
    "print(f\"| F-Score (0.5)    | {stats['f_5']['average_threshold']:.3f}      | {stats['f_5']['median_threshold']:.3f}    | {stats['f_5']['threshold_deviation']:.3f}        |\")\n",
    "print(f\"| F-Score (0.3)    | {stats['f_3']['average_threshold']:.3f}      | {stats['f_3']['median_threshold']:.3f}    | {stats['f_3']['threshold_deviation']:.3f}        |\")\n",
    "print(f\"| F-Score (0.1)    | {stats['f_1']['average_threshold']:.3f}      | {stats['f_1']['median_threshold']:.3f}    | {stats['f_1']['threshold_deviation']:.3f}        |\")\n",
    "\n",
    "\n",
    "for m in [\"f_5\", \"f_3\", \"f_1\"]:\n",
    "    \n",
    "    print(f\"\\n### Average {measure_labels[m]}\")\n",
    "    print(\"\\n\")\n",
    "    print(f\"|                  | Average               |      |\")\n",
    "    print(f\"| ---------------- | --------------------- | ---- |\")\n",
    "    print(f\"|                  | Training              | Test |\")\n",
    "    print(f\"| Threshold        | {stats[m]['average_threshold']:.3f}      |      |\")\n",
    "    print(f\"| Precision        | {stats[m]['average_precision_training']:.3f}      | {stats[m]['average_precision_test']:.3f}  |\")\n",
    "    print(f\"| Recall           | {stats[m]['average_recall_training']:.3f}      | {stats[m]['average_recall_test']:.3f}  |\")\n",
    "    print(f\"| {m}              | {stats[m]['average_measure_training']:.3f}      | {stats[m]['average_measure_test']:.3f}  |\")\n",
    "    print(f\"| score_deviation  | {stats[m]['score_deviation']:.3f}      | {stats[m]['score_deviation']:.3f}  |\")\n",
    "    print(f\"| random_{m}       | {stats[m]['average_random_training']:.3f}      | {stats[m]['average_random_test']:.3f}  |\")\n",
    "    print(f\"| frequency_{m}    | {stats[m]['average_frequency_training']:.3f}      | {stats[m]['average_frequency_test']:.3f}  |\")\n",
    "    print(f\"| minority_{m}     | {stats[m]['average_minority_training']:.3f}      | {stats[m]['average_minority_test']:.3f}  |\")\n",
    "    print(f\"| majority_{m}     | {stats[m]['average_majority_training']:.3f}      | {stats[m]['average_majority_test']:.3f}  |\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../data/outputs/cross_validation/results/english{model_name}_{similarity_measure}.json\", \"w\") as f:\n",
    "    json.dump(model_results, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#similarity_measure = \"cosine\"\n",
    "#similarity_measure = \"spearmanr\"\n",
    "#model_name = \"gloss[2]\"\n",
    "\n",
    "#with open(f\"../cross_validation/results/swedish_10_{model_name}_{similarity_measure}.json\", \"r\") as f:\n",
    "#    model_results = json.load(f)\n",
    "\n",
    "\n",
    "if similarity_measure == \"cosine\":\n",
    "    sim_measure = \"Cosine Similarity\"\n",
    "else:\n",
    "    sim_measure = \"Spearman's ρ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(model_results[0][\"results\"][0])\n",
    "pr_curves = []\n",
    "\n",
    "for round in model_results:\n",
    "    rl = round[\"fold_split\"]\n",
    "    round = round[\"results\"] \n",
    "    for fold in round:\n",
    "        prs = []\n",
    "        for t in fold[\"f_5\"][\"training\"]: # use f_5 for pr curve\n",
    "            prs.append((t[\"precision\"], t[\"recall\"]))\n",
    "\n",
    "        pr_curves.append({\n",
    "            \"round\": rl,\n",
    "            \"fold\": fold,\n",
    "            \"prs\": prs\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "#import seaborn as sns\n",
    "\n",
    "# colors with rgba\n",
    "color_map = {\n",
    "    0: '#F7B267', # red\n",
    "    1: '#F79D65',\n",
    "    2: '#F4845F',\n",
    "    3: '#F27059',\n",
    "    4: '#F25C54'\n",
    "}\n",
    "\n",
    "for round in range(1, 2):\n",
    "    f_5_scores = []\n",
    "    f_3_scores = []\n",
    "    f_1_scores = []\n",
    "\n",
    "    # plot pr curve\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    for pr in pr_curves:\n",
    "        if pr[\"round\"] == round:\n",
    "            for p in pr[\"prs\"]:\n",
    "                # exclude tuples with 0 or 1 values\n",
    "                if p[0] == 0 or p[1] == 0 or p[0] == 1 or p[1] == 1:\n",
    "                    continue            \n",
    "                f =  pr[\"fold\"][\"f_5\"][\"test\"][\"split_label\"][0]\n",
    "\n",
    "                ax.scatter(p[1], p[0], color=color_map[f])\n",
    "\n",
    "    # add legend\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], marker='o', color='w', label='Fold 1', markerfacecolor=color_map[0], markersize=5),\n",
    "        Line2D([0], [0], marker='o', color='w', label='Fold 2', markerfacecolor=color_map[1], markersize=5),\n",
    "        Line2D([0], [0], marker='o', color='w', label='Fold 3', markerfacecolor=color_map[2], markersize=5),\n",
    "        Line2D([0], [0], marker='o', color='w', label='Fold 4', markerfacecolor=color_map[3], markersize=5),\n",
    "        Line2D([0], [0], marker='o', color='w', label='Fold 5', markerfacecolor=color_map[4], markersize=5),\n",
    "    ]\n",
    "\n",
    "    ax.legend(handles=legend_elements, loc='lower left')\n",
    "    #ax.set_color_cycle(sns.color_palette(\"Paired\", 10))\n",
    "\n",
    "    # display round and model\n",
    "    model = f\"{model_name[0].upper()}{model_name.split('[')[1].split(']')[0]} - {sim_measure}\"\n",
    "\n",
    "    plt.title(\"Precision-Recall Curve\", fontsize=12, fontweight=\"bold\", pad=20)\n",
    "    plt.suptitle(f\"Round {round} - {model}\", fontsize=10, y=1.0)   \n",
    "\n",
    "    ax.set_xlabel(\"Recall\")\n",
    "    ax.set_ylabel(\"Precision\")\n",
    "\n",
    "    fig1 = plt.gcf()\n",
    "    plt.show()\n",
    "\n",
    "    # save plot\n",
    "    #fig1.savefig(f\"../cross_validation/results/pr_curve_{model_name}_{sim_measure}_round_{round}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import font_manager\n",
    "\n",
    "\n",
    "def plot_precision_recall_vs_threshold(precision, recall, thresholds, round, top):\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.plot(thresholds[0], precision[0], \"--\", label=\"Precision\", color='#465775')\n",
    "    plt.plot(thresholds[0], recall[0], \"-\", label=\"Recall\", color='#EF6F6C')\n",
    "    for i in range(1, len(precision)):\n",
    "        plt.plot(thresholds[i], precision[i], \"--\", color='#465775')\n",
    "        plt.plot(thresholds[i], recall[i], \"-\", color='#EF6F6C')\n",
    "    \n",
    "    for t in top:\n",
    "        plt.axvline(x=t[0], color=t[2], linestyle=\":\", label=f\"{t[1]} Threshold\", alpha=0.8)\n",
    "\n",
    "    tick_scores = [0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "    scores = [t[0] for t in top]\n",
    "    too_close = []\n",
    "    for t in tick_scores:\n",
    "        if any(abs(t - s) < 0.1 for s in scores):\n",
    "            too_close.append(t)\n",
    "\n",
    "    for t in too_close:\n",
    "        tick_scores.remove(t)\n",
    "\n",
    "    tick_scores.append(scores[0])\n",
    "    for i in range(1, len(scores)):\n",
    "        if abs(scores[i] - scores[i-1]) < 0.02:\n",
    "            continue\n",
    "        tick_scores.append(scores[i])\n",
    "        \n",
    "    #tick_scores.extend(scores)\n",
    "    ax.set_xticks(tick_scores, minor=False)\n",
    "\n",
    "    plt.title(\"Precision-Recall vs Threshold Curve\", fontsize=12, fontweight=\"bold\", pad=10)\n",
    "    plt.suptitle(f\"Round {round} - {model}\", fontsize=10, y=1.0)\n",
    "\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.ylabel(\"Precision-Recall\")\n",
    "\n",
    "    font = font_manager.FontProperties(family='Rubik',\n",
    "                                   style='normal', size=10)\n",
    "\n",
    "    plt.legend(loc=\"lower left\", prop=font)\n",
    "    plt.ylim([0,1])\n",
    "    plt.xlim([0,1])\n",
    "\n",
    "    fig1 = plt.gcf()\n",
    "    plt.show()\n",
    "\n",
    "    #fig1.savefig(f\"../cross_validation/results/pr_threshold_curve_{model_name}_{sim_measure}_round_{round}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(model_results[0][\"results\"][0])\n",
    "\n",
    "for r in model_results:\n",
    "    rl = r[\"fold_split\"] # round label\n",
    "    fold = r[\"results\"]\n",
    "    pss = []\n",
    "    rss = []\n",
    "    tss = []\n",
    "    top5ts = []\n",
    "    top3ts = []\n",
    "    top1ts = []\n",
    "    for f in fold:\n",
    "        ps = []\n",
    "        rs = []\n",
    "        ts = []\n",
    "        results = f[\"train_results\"]\n",
    "        for r in results:\n",
    "            # continue if any is 0\n",
    "            if r[\"precision\"] == 0 or r[\"recall\"] == 0:\n",
    "                continue\n",
    "            ps.append(r[\"precision\"])\n",
    "            rs.append(r[\"recall\"])\n",
    "            ts.append(r[\"threshold\"])\n",
    "        \n",
    "        pss.append(ps)\n",
    "        rss.append(rs)\n",
    "        tss.append(ts)\n",
    "\n",
    "\n",
    "        top5ts.append(f[\"f_5\"][\"test\"][\"threshold\"])\n",
    "        top3ts.append(f[\"f_3\"][\"test\"][\"threshold\"])\n",
    "        top1ts.append(f[\"f_1\"][\"test\"][\"threshold\"])\n",
    "    \n",
    "    # median threshold\n",
    "    top5 = (np.median(top5ts), \"F₀.₅\", '#724CF9')\n",
    "    top3 = (np.median(top3ts), \"F₀.₃\", '#CA7DF9')\n",
    "    top1 = (np.median(top1ts), \"F₀.₁\", '#F896D8')\n",
    "    \n",
    "    plot_precision_recall_vs_threshold(pss, rss, tss, rl, [top5, top3, top1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
