{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Data\n",
    "This notebook samples word usages from a corpus for the headwords of a dictionary.\n",
    "It is used to create the sample data for the first human annotation.\n",
    "Since searching every sentence for all headwords is very time consuming, the notebook samples a number of headwords and then searches for usages of these headwords in the corpus.\n",
    "It keeps all usages of the sampled headwords so further processing, i.e., reducing the number of usages is done in another notebook.\n",
    "\n",
    "### Usage\n",
    "Various parameters need to be set in the second cell of the notebook, including:\n",
    "- `input_file_headwords`: path to the dictionary file\n",
    "- `input_file_sentences`: path to the corpus file\n",
    "- `number_of_words`: number of headwords to be sampled (Only needs to be large enough to find enough headwords with usages to reach the desired sample size.)\n",
    "- `size_of_sample`: number of headwords to find usages for\n",
    "- `max_senses`: maximum number of senses for a headword to be considered (to not inflate the human annotation with headwords with many senses)\n",
    "\n",
    "The notebook outputs a json file with the sampled headwords and their usages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from random import randrange\n",
    "from Levenshtein import distance as lev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 56266\n"
     ]
    }
   ],
   "source": [
    "size_of_sample = 170\n",
    "number_of_words = 2000\n",
    "seed = randrange(100000)\n",
    "print(f\"seed: {seed}\")\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ENGLISH\n",
    "#input_file_headwords = '../data/dictionaries/wordnet_sense_id.json'\n",
    "#input_file_sentences = '../data/corpora/PROCESSED_eng_news_2020_1M-sentences.json'\n",
    "#input_file_sentences = '../data/corpora/PROCESSED_ccoha1.json'\n",
    "\n",
    "## SWEDISH\n",
    "input_file_headwords = '../data/dictionaries/sw_dict_sense_id.json'\n",
    "input_file_sentences = '../data/corpora/PROCESSED_swe_news_2022_1M-sentences.json'\n",
    "#input_file_sentences = '../data/corpora/PROCESSED_kubhist2a.json'\n",
    "\n",
    "max_senses = 5\n",
    "\n",
    "\n",
    "output_file = f'../data/outputs/SAMPLED_{input_file_sentences.split(\"/\")[-1].split(\".\")[0]}.json'\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds a sublist in a list\n",
    "def find_multiword(word, sentence):\n",
    "    ln = len(word)\n",
    "    for i in range(len(sentence) - ln + 1):\n",
    "        if all(word[j] == sentence[i+j] for j in range(ln)):\n",
    "            return i\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds all positions of a word in a sentence\n",
    "def find_multiple_occurences(word, sentence):\n",
    "    return [i for i in range(len(sentence)) if sentence.startswith(word, i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of headwords: 39069\n"
     ]
    }
   ],
   "source": [
    "# load headwords with less than max_senses senses\n",
    "headwords = []\n",
    "\n",
    "with open(input_file_headwords, encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "    for d in data:\n",
    "        if len(d['entries']) > max_senses: # skip words with too many senses\n",
    "            continue\n",
    "        if all(d['sense'] == \"\" for d in d['entries']): # skip words without sense definitions (as they are needed for human evaluation)\n",
    "            continue\n",
    "        if '_' in d['key']: # split multiword expressions\n",
    "            headwords.append(d['key'].split('_'))\n",
    "        else:\n",
    "            headwords.append(d['key'].split('-'))\n",
    "\n",
    "    print(f\"number of headwords: {len(headwords)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a random sample of headwords\n",
    "headword_sample = []\n",
    "\n",
    "for i in range(number_of_words):\n",
    "    index = random.randint(0, len(headwords)-1) # pick a random headword\n",
    "    headword_sample.append(headwords.pop(index)) # remove it from the list and add it to the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find sentences containing the headwords\n",
    "with open(input_file_sentences, encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "    matches = {\"file\": f\"{input_file_sentences.split('/')[-1]}\", \"entries\": {}}\n",
    "    for d in data:\n",
    "        if len(matches[\"entries\"]) >= size_of_sample: # stop when sample size is reached\n",
    "            break\n",
    "        for headword in headword_sample: # for all headwords in sample\n",
    "                       \n",
    "            first_word = headword[0]\n",
    "            lemma = '_'.join(headword)\n",
    "\n",
    "            if first_word in d['lemmatized']:\n",
    "                if len(headword) > 1: # if multiword expression\n",
    "                    if find_multiword(headword, d['lemmatized']) == -1:\n",
    "                        continue\n",
    "                if matches[\"entries\"].get(lemma) == None: # if first entry\n",
    "                    matches[\"entries\"][lemma] = [{\"sentence_id\":d[\"sentence_number\"], \"entry\": {\"sentence\": d['sentence'], \"tokenized\": d[\"tokenized\"], \"lemmatized\": d['lemmatized'], \"pos_tag\": d['pos_tag']}}]\n",
    "                else:\n",
    "                    is_duplicate = False\n",
    "                    for usage in matches[\"entries\"][lemma]:\n",
    "                        if lev(usage[\"entry\"][\"sentence\"], d['sentence']) < 10: # if another usage is similar\n",
    "                            is_duplicate = True\n",
    "                    if not is_duplicate:\n",
    "                        matches[\"entries\"][lemma].append({\"sentence_id\":d[\"sentence_number\"], \"entry\": {\"sentence\": d['sentence'], \"tokenized\": d[\"tokenized\"], \"lemmatized\": d['lemmatized'], \"pos_tag\": d['pos_tag']}})\n",
    "\n",
    "results = {\"seed\": seed, \"headwords_searched\": number_of_words, \"headwords_found\": 0, \"max_senses\": max_senses, \"entries\": {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct result file\n",
    "for lemma in matches['entries']:\n",
    "    for s in range(len(matches['entries'][lemma])):\n",
    "\n",
    "        sentence_number = matches['entries'][lemma][s]['sentence_id']\n",
    "        \n",
    "        # retrieve sentences\n",
    "        sentence = matches['entries'][lemma][s]['entry']['sentence']\n",
    "        tokenized = matches['entries'][lemma][s]['entry']['tokenized']\n",
    "        lemmatized = matches['entries'][lemma][s]['entry']['lemmatized']\n",
    "        pos_tag = matches['entries'][lemma][s]['entry']['pos_tag']\n",
    "\n",
    "        # position of headword in lemmatized sentence\n",
    "        lemma_index_1 = find_multiword(lemma.split('_'), lemmatized)\n",
    "        lemma_index_2 = lemma_index_1 + len(lemma.split('_')) - 1\n",
    "\n",
    "        # construct multiwords\n",
    "        tokens = []\n",
    "        for i in range(lemma_index_1, lemma_index_2 + 1):\n",
    "            tokens += [tokenized[i]]\n",
    "        multiwords = [' '.join(tokens)]\n",
    "        multiwords.append('-'.join(tokens))\n",
    "        multiwords.append('/'.join(tokens))\n",
    "        multiwords.append(', '.join(tokens))\n",
    "        multiwords.append(' , '.join(tokens))\n",
    "\n",
    "        # get pos information\n",
    "        pos_tags = []\n",
    "        for i in range(lemma_index_1, lemma_index_2 + 1):\n",
    "            pos_tags += [pos_tag[i]]\n",
    "        \n",
    "        # sanity check if any possible word usage is in sentence\n",
    "        if all(multiword not in sentence for multiword in multiwords):\n",
    "            print(f\"ERROR: {word_usages[0]} not in {sentence}\")\n",
    "            continue\n",
    "    \n",
    "        # find used multiword in sentence\n",
    "        for w in multiwords:\n",
    "            if w in sentence:\n",
    "                multiword = w\n",
    "                break\n",
    "    \n",
    "        sentence_indieces = []\n",
    "\n",
    "        # find all occurences of word in sentence\n",
    "        if sentence.startswith(multiword):\n",
    "            sentence_indieces = [0]\n",
    "        \n",
    "        for i in range(1, len(sentence) - len(multiword) + 1):\n",
    "            if sentence.startswith(multiword, i):\n",
    "                if not sentence[i-1].isalpha():\n",
    "                    sentence_indieces.append(i)\n",
    "        if len(sentence_indieces) == 0:\n",
    "            print(f\"ERROR: {multiword} not in {sentence}\")\n",
    "            continue\n",
    "\n",
    "        sentence_index_1 = sentence_indieces[0]\n",
    "        # go through all occurences\n",
    "        for i in sentence_indieces:\n",
    "            # check if character after word usage is a [a-z]\n",
    "            if i + len(multiword) < len(sentence):\n",
    "                if not sentence[i + len(multiword)].isalpha(): # if not, continue\n",
    "                    sentence_index_1 = i\n",
    "                    break\n",
    "        sentence_index_2 = sentence_index_1 + len(multiword)\n",
    "\n",
    "        if lev(multiword, sentence[sentence_index_1:sentence_index_2]) > 0:\n",
    "            print(f\"ERROR: word usage {multiword} not at index {[sentence_index_1, sentence_index_2]}\\nInstead: {sentence[sentence_index_1:sentence_index_2]}\\n\")\n",
    "\n",
    "\n",
    "        if results[\"entries\"].get(lemma) == None:\n",
    "            results[\"entries\"][lemma] = []\n",
    "\n",
    "        results['entries'][lemma].append({\n",
    "            \"word_usage\": multiword,\n",
    "            \"lemma\": lemma,\n",
    "            \"pos\": pos_tags,\n",
    "            \"identifier\": f\"{matches['file']}-{sentence_number}-{lemma_index_1}:{lemma_index_2}\",\n",
    "            \"sentence\": sentence,\n",
    "            \"tokenized\": tokenized,\n",
    "            \"lemmatized\": lemmatized,\n",
    "            \"character_index_sentence\": f\"{sentence_index_1}:{sentence_index_2}\",\n",
    "            \"index_lemmatized\": f\"{lemma_index_1}:{lemma_index_2}\"\n",
    "            })\n",
    "    results[\"headwords_found\"] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['flytt', 'förstärka', 'roman', 'ockupera', 'undersökning'] (170)\n",
      "170\n"
     ]
    }
   ],
   "source": [
    "hw = [' '.join(h) for h in headword_sample]\n",
    "found_lemmas = [l for l in results['entries']]\n",
    "print(f\"{found_lemmas[:5]} ({len(set(found_lemmas))})\")\n",
    "print(len(results['entries']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
