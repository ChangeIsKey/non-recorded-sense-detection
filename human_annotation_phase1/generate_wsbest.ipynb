{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate WSBest\n",
    "This notebook generates all data needed for the WSBest annotation on PhiTag:\n",
    "- `senses.tsv` all senses that appear in the sample\n",
    "- `usages.tsv` all usages that appear in the sample\n",
    "- `instances.tsv` all possible combinations of senses and usages\n",
    "\n",
    "### Usage\n",
    "In the second cell, the variables `dictionary_file` and `sample_usage_file` need to be set to the respective files.\n",
    "\n",
    "For `sample_usage_file`, it is intended to use the reduced sample file to create an appropriated sample for the WSBest annotation. The reduced sample file can be created with the notebook `reduce_sample.ipynb`.\n",
    "\n",
    "Since the notebook creates a lot of files, subfolders are created for eache type of data. For this, the variable `target_dir` needs to be set to the desired target directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_file = \"../data/dictionaries/wordnet_sense_id.json\"\n",
    "#dictionary_file = \"../data/dictionaries/sw_dict_sense_id.json\"\n",
    "\n",
    "sample_usage_file = \"../data/outputs/REDUCED_SAMPLE_eng_news_2020_1M-sentences.json\"\n",
    "#sample_usage_file = \"../data/outputs/REDUCED_SAMPLE_ccoha1.json\"\n",
    "\n",
    "#sample_usage_file = \"../data/outputs/REDUCED_SAMPLE_swe_news_2022_1M-sentences.json\"\n",
    "#sample_usage_file = \"../data/outputs/REDUCED_SAMPLE_kubhist2a.json\"\n",
    "\n",
    "target_dir = \"ws_best_en_modern\"\n",
    "#target_dir = \"ws_best_en_historical\"\n",
    "#target_dir = \"ws_best_sw_modern\"\n",
    "#target_dir = \"ws_best_sw_historical\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sample, extract lemmas, usages and config data\n",
    "with open(sample_usage_file, \"r\") as f:\n",
    "    sample_usage = json.load(f)\n",
    "\n",
    "    seed = sample_usage[\"seed\"]\n",
    "    sample_size = sample_usage[\"sample_size\"]\n",
    "    max_senses = sample_usage[\"max_senses\"]\n",
    "\n",
    "    usages = []\n",
    "    lemmas = []\n",
    "\n",
    "    # append each usage to usages list, including dataID, context, indices_target_token, indices_target_sentence and lemma\n",
    "    for use in sample_usage[\"sample\"]:\n",
    "        dataID = use[\"identifier\"]\n",
    "        context = use[\"sentence\"]\n",
    "        indices_target_token = use[\"character_index_sentence\"]\n",
    "        indices_target_sentence = f\"0:{len(context)}\"\n",
    "        lemma = use[\"lemma\"]\n",
    "\n",
    "        lemmas.append(lemma)\n",
    "        usages.append({\"dataID\": dataID,\n",
    "                       \"context\": context,\n",
    "                       \"indices_target_token\": indices_target_token,\n",
    "                       \"indices_target_sentence\": indices_target_sentence,\n",
    "                       \"lemma\": lemma})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dictionary and extract senses\n",
    "with open(dictionary_file, \"r\") as f:\n",
    "    dictionary = json.load(f)\n",
    "\n",
    "    senses = []\n",
    "\n",
    "    for entry in dictionary:\n",
    "        # sanity check if headword has not more than max_senses senses\n",
    "        if entry[\"key\"] in lemmas and not (len(entry[\"entries\"]) > max_senses): \n",
    "            for sense in entry[\"entries\"]:\n",
    "                if sense[\"sense\"] == \"\": # skip senses with empty definition\n",
    "                    continue\n",
    "                senseID = sense[\"identifier\"]\n",
    "                definition = sense[\"sense\"]\n",
    "                lemma = entry[\"key\"]\n",
    "\n",
    "                senses.append({\"senseID\": senseID,\n",
    "                               \"definition\": definition,\n",
    "                               \"lemma\": lemma})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create instances\n",
    "instances = []\n",
    "\n",
    "for use in usages:\n",
    "    dataID = use[\"dataID\"]\n",
    "    lemma_usage = use[\"lemma\"]\n",
    "\n",
    "    for sense in senses:\n",
    "        if sense[\"lemma\"] == lemma_usage:\n",
    "            senseID = sense[\"senseID\"]\n",
    "\n",
    "            instanceID = f\"{dataID}-{senseID}\"\n",
    "            dataIDs = f\"{dataID},{senseID}\"\n",
    "            label_set = '0,1'\n",
    "\n",
    "            instances.append({\"instanceID\": instanceID,\n",
    "                              \"dataIDs\": dataIDs,\n",
    "                              \"label_set\": label_set,\n",
    "                              \"non_label\": \"-\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write usages as single file\n",
    "with open(f\"../data/outputs/annotation_phase_1/{target_dir}/usages.tsv\", \"w\") as f:\n",
    "    writer = csv.writer(f, delimiter='\\t', quotechar='\\\\')\n",
    "    header = usages[0].keys()\n",
    "    writer.writerow(header)\n",
    "\n",
    "    for usage in usages:\n",
    "        writer.writerow(usage.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write senses as single file\n",
    "with open(f\"../data/outputs/annotation_phase_1/{target_dir}/senses.tsv\", \"w\") as f:\n",
    "    writer = csv.writer(f, delimiter='\\t')\n",
    "    header = senses[0].keys()\n",
    "    writer.writerow(header)\n",
    "\n",
    "    for sense in senses:\n",
    "        writer.writerow(sense.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write instances as single file\n",
    "with open(f\"../data/outputs/annotation_phase_1/{target_dir}/instances.tsv\", \"w\") as f:\n",
    "    writer = csv.writer(f, delimiter='\\t')\n",
    "    header = instances[0].keys()\n",
    "    writer.writerow(header)\n",
    "\n",
    "    for instance in instances:\n",
    "        writer.writerow(instance.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate config.json\n",
    "with open(f\"../data/outputs/annotation_phase_1/{target_dir}/config.json\", \"w\") as f:\n",
    "    config = {\n",
    "        \"seed\": seed,\n",
    "        \"sample_size\": sample_size\n",
    "    }\n",
    "    json.dump(config, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
