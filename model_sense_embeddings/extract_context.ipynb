{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract context\n",
    "Extract context from a dictionary for embedding creation. If the context does not contain the targeted headword, different options are available to add it:\n",
    "\n",
    "### Options\n",
    "|option|description|example|\n",
    "|---|---|---|\n",
    "| 0 | leaves the examples as they are | a poor salary |\n",
    "| 1 | HEADWORD: CONTEXT | inadequate: a poor salary |\n",
    "| 2 | CONTEXT (HEADWORD) | a poor salary (inadequate) |\n",
    "| 3 | CONTEXT, i.e., HEADWORD | a poor salary, i.e. inadequate |\n",
    "| 4 | replace word | a inadequate salary |\n",
    "\n",
    "### Usage\n",
    "Set the desired dictionary file in the second cell of the notebook in the variable `dictionary_file`.\n",
    "Choose the desired option for context replacement with the variable `transformation`.\n",
    "\n",
    "### Disclaimer\n",
    "This notebook was changed for each data type. The current version is for English gloss data only.\n",
    "The imported helper function `get_indieces` does not work for Swedish context. \n",
    "Furthermore, the third code cell extracts the glosses from the dictionary file and was adjusted for the extraction of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import random\n",
    "from nltk.corpus import wordnet as wn\n",
    "from lemminflect import *\n",
    "from random import randrange\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../helper_scripts')\n",
    "\n",
    "from index_finder import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_file = \"../data/dictionaries/wordnet_sense_id.json\"\n",
    "#dictionary_file = \"../data/dictionaries/sw_dict_sense_id.json\"\n",
    "\n",
    "transformation = 3\n",
    "\n",
    "output_file = f\"../data/outputs/dictionary_context/{dictionary_file.split('/')[-1].split('.')[0]}/gloss[{transformation}].json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms the context\n",
    "def transform_context(lemma, context, sense_id):\n",
    "    lemma = lemma.replace(\" \", \"_\")\n",
    "    match transformation:\n",
    "        case 0:\n",
    "            return context\n",
    "        case 1:\n",
    "            return f\"{lemma.replace('_', ' ')}: {context}\"\n",
    "        case 2:\n",
    "            return f\"{context} ({lemma.replace('_', ' ')})\"\n",
    "        case 3:\n",
    "            return f\"{context}, i.e., {lemma.replace('_', ' ')}\"\n",
    "        case 4:\n",
    "            synsets = []\n",
    "            # get all synsets of the lemma\n",
    "            for s in wn.synsets(lemma):\n",
    "                if s.lemmas()[0].name().lower() == lemma:\n",
    "                    synsets.append(s)\n",
    "\n",
    "            target_synset = synsets[sense_id].lemma_names() # get headwords of the synsets\n",
    "            target_synset = set(target_synset) # remove duplicates\n",
    "\n",
    "            \n",
    "            for l in target_synset:\n",
    "                target = get_indieces(context, l)[\"word\"] # search for the headword in the context\n",
    "                if target != [-1, -1]:\n",
    "                    word = context[target[0]:target[1]]\n",
    "                    return context.replace(word, lemma)\n",
    "                    \n",
    "            return f\"{context} ({lemma.replace('_', ' ')})\" # if no headword is found, apply strategy 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gloss: 117659\n",
      "Gloss without target: 0\n",
      "Gloss without target after transformation: 0\n"
     ]
    }
   ],
   "source": [
    "# extract gloss from dictionary entry\n",
    "with open(dictionary_file) as f:\n",
    "    dictionary = json.load(f)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # keep track of statistics\n",
    "    total_gloss = 0\n",
    "    gloss_without_target = 0\n",
    "    gloss_without_target_post_transformation = 0\n",
    "\n",
    "    for entry in dictionary: # can be run on a subset of the dictionary to test functionality with low runtime \"dictionary[:100]\"\n",
    "        lemma = entry[\"key\"]\n",
    "        results[lemma] = []\n",
    "        for sense in entry[\"entries\"]:\n",
    "            sense_id = int(sense[\"identifier\"].split(\".\")[-1].split('-')[-1]) # last digit of identifier is the position of the sense in the dictionary entry\n",
    "            gloss = sense[\"sense\"]\n",
    "            total_gloss += 1\n",
    "            gloss = transform_context(lemma, gloss, sense_id) # transform the gloss\n",
    "            try:\n",
    "                match transformation:\n",
    "                    case 0: # no transformation\n",
    "                        target = get_indieces(gloss, lemma)[\"word\"]\n",
    "                    case 1: # add lemma at the beginning\n",
    "                        pos1 = 0\n",
    "                        pos2 = len(lemma.replace(\"_\", \" \"))\n",
    "                        target = [pos1, pos2]\n",
    "                    case 2: # add lemma at the end with brackets\n",
    "                        pos1 = len(gloss) - len(lemma.replace(\"_\", \" \")) - 2 # -2 because of the brackets\n",
    "                        pos2 = pos1 + len(lemma.replace(\"_\", \" \")) \n",
    "                        target = [pos1, pos2]\n",
    "                    case 3: # add lemma at the end with i.e.,\n",
    "                        pos1 = gloss.index(\", i.e.,\") + 8 # +8 because of the ', i.e.,'\n",
    "                        pos2 = pos1 + len(lemma.replace(\"_\", \" \"))\n",
    "                        target = [pos1, pos2]\n",
    "                    case 4: # replace the headword of the sense with the lemma\n",
    "                        target = get_indieces(gloss, lemma)[\"word\"]\n",
    "            except ValueError:\n",
    "                print(f\"lemma: {lemma} not found in transformed gloss: {gloss}\")\n",
    "                target = [-1, -1]\n",
    "            \n",
    "            results[lemma].append({\n",
    "                \"sense\": sense[\"identifier\"],\n",
    "                \"usages\":{\n",
    "                    \"usage\": gloss,\n",
    "                    \"target\": target\n",
    "                    } \n",
    "                })\n",
    "            \n",
    "            # print progress\n",
    "            if len(results) % 100 == 0 and len(results) > 0:\n",
    "                print(f\"{round(100 * len(results) / len(dictionary), 2)}%\", end=\"\\r\")\n",
    "            \n",
    "    print(f\"Total gloss: {total_gloss}\")\n",
    "    print(f\"Gloss without target: {gloss_without_target}\")\n",
    "    print(f\"Gloss without target after transformation: {gloss_without_target_post_transformation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(results, f, indent=4, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
