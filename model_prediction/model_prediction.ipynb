{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model prediction\n",
    "This notebook calculates the similarity between a word usage and all eligible senses.\n",
    "It then predicts whether the word usage is represented by a sense based on a threshold.\n",
    "\n",
    "### Usage\n",
    "Adjust the variables in cell 2:\n",
    "Set the file paths to the sampled word usage file from the `sample_data.ipynb` notebook and the sense embeddings of the desired model.\n",
    "Set the threshold to the tuned value and the `sim_measure` variable to either `cosine` or `spearmanr`.\n",
    "Set the spaCy model to the desired language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import ast\n",
    "import json\n",
    "import sys\n",
    "import spacy\n",
    "import re\n",
    "import scipy\n",
    "\n",
    "sys.path.insert(1, '../xl-lexeme/WordTransformer')\n",
    "\n",
    "from InputExample import InputExample\n",
    "from WordTransformer import WordTransformer\n",
    "\n",
    "model = WordTransformer('pierluigic/xl-lexeme') # load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set file paths\n",
    "sentence_file = \"../data/outputs/annotation_phase_2/SAMPLE_eng_news_2020_1M-sentences[150000].csv\"\n",
    "model_embeddings = \"../data/outputs/sense_embeddings/english/wordnet_sense_id_gloss_[3]_embeddings.json\"\n",
    "\n",
    "threshold = 0.411\n",
    "sim_measure = \"cosine\"\n",
    "\n",
    "dictionary_file = \"../data/dictionaries/wordnet_sense_id.json\"\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "#nlp = spacy.load(\"sv_core_news_sm\")\n",
    "\n",
    "# output file\n",
    "corpus_type = sentence_file.split('/')[-1].split('[')[0]\n",
    "model_name = model_embeddings.split('/')[-1].split('.')[0]\n",
    "output_file = f\"../data/outputs/model_predictions/{corpus_type}-{model_name}_predictions.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sentence sample\n",
    "df = pd.read_csv(sentence_file, sep=\"\\t\").sample(100000, random_state=42)\n",
    "df[\"sentence\"] = df[\"sentence\"].apply(lambda x: re.sub(r\"\\s+\", \" \", x).strip()) # remove multiple whitespaces\n",
    "lemmas = list(set(list(itertools.chain.from_iterable([ast.literal_eval(l) for l in df[\"lemmas\"]])))) # get unique lemmas\n",
    "print(\"Number of sentences: \", len(df))\n",
    "print(\"Number of lemmas: \", len(lemmas))\n",
    "display(df.head(3))\n",
    "print(lemmas[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embeddings\n",
    "with open(model_embeddings, \"r\") as f:\n",
    "    embeddings = json.load(f)\n",
    "    possible_lemmas = list(set(lemmas).intersection(set(embeddings.keys())))\n",
    "\n",
    "print(\"possible lemmas \", len(possible_lemmas))\n",
    "print(possible_lemmas[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all lemmas from the lists that are not possible to predict\n",
    "df['lemmas'] = df['lemmas'].apply(lambda x: [l for l in ast.literal_eval(x) if l in possible_lemmas])\n",
    "df = df[df[\"lemmas\"].apply(lambda x: len(x) > 0)]\n",
    "print(\"Number of sentences after removing lemmas not in embeddings: \", len(df))\n",
    "display(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(a, b):\n",
    "    if sim_measure == \"cosine\":\n",
    "        return scipy.spatial.distance.cosine(a, b)\n",
    "    elif sim_measure == \"spearmanr\":\n",
    "        return scipy.stats.spearmanr(a, b)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarities(lemma, embedding):\n",
    "    senses = [s for s in embeddings[lemma] if s[\"usages\"] != []] \n",
    "    closest_similarity = 0\n",
    "    closest_sense = None\n",
    "    closest_pos = None\n",
    "\n",
    "    for sense in senses:\n",
    "        similarity = similarity(sense[\"embedding\"], embedding)\n",
    "        if similarity > closest_similarity:\n",
    "            closest_sense = sense[\"sense\"] \n",
    "            closest_similarity = similarity\n",
    "   \n",
    "    closest_pos = [t.pos_ for t in nlp(lemma)][0]\n",
    "    return (closest_sense, closest_similarity, closest_pos) # 1 if unknown, 0 if known"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_sense_lemmas = []\n",
    "with open(dictionary_file, \"r\") as f:\n",
    "    dict_sense_id = json.load(f)\n",
    "\n",
    "    for lemma in dict_sense_id:\n",
    "        if len([s for s in lemma[\"entries\"]]) > 10:\n",
    "            high_sense_lemmas.append(lemma[\"key\"])\n",
    "\n",
    "print(\"Number of lemmas with more than 10 senses: \", len(high_sense_lemmas))\n",
    "display(high_sense_lemmas[:3])\n",
    "print(lemma[\"key\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre fill the unassigned usages dict with all lemmas and senses\n",
    "all_lemmas = list(set(list(itertools.chain.from_iterable([l for l in df[\"lemmas\"]]))))\n",
    "print(len(all_lemmas))\n",
    "print(all_lemmas[:10])\n",
    "unassigned_usages = {}\n",
    "\n",
    "for lemma in dict_sense_id:\n",
    "    if lemma[\"key\"] in all_lemmas:\n",
    "\n",
    "        if lemma[\"key\"] in high_sense_lemmas:\n",
    "            continue\n",
    "        senses = [{\"sense:\": s[\"identifier\"], \"definition\": s[\"sense\"]} for s in lemma[\"entries\"]]\n",
    "        unassigned_usages[lemma[\"key\"]] = {\"senses\": senses, \"usages\": []}\n",
    "\n",
    "display(unassigned_usages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = df[[\"sentence\", \"lemmas\"]].apply(tuple, axis=1)\n",
    "#display(instances.head(3))\n",
    "inst = sum([len(l) for s, l in instances])\n",
    "print(\"Number of instances: \", inst)\n",
    "i = 0\n",
    "\n",
    "\n",
    "for sentence, lemmas in instances: # for all sentence lemma pairs\n",
    "    doc = nlp(sentence) \n",
    "    lemmatized = [(l.lemma_, l.text) for l in doc]\n",
    "    for lemma in lemmas:\n",
    "        if lemma in high_sense_lemmas: # skip lemmas with more than 10 senses\n",
    "            continue\n",
    "        if lemma in unassigned_usages.keys() and len(unassigned_usages[lemma][\"usages\"]) > 500: # only look at 500 usages per lemma\n",
    "            continue\n",
    "        if lemma == ' ': # skip empty lemmas\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            token = [l[1] for l in lemmatized if l[0] == lemma][0] # get the first occurence of the lemma\n",
    "            target = [sentence.index(token), sentence.index(token)+len(token)]\n",
    "        except:\n",
    "            continue   \n",
    "\n",
    "        embedding = model.encode(InputExample(texts=sentence, positions=target))\n",
    "        sims = get_similarities(lemma, embedding)\n",
    "        if 0 < sims[1] < threshold: # tuned threshold\n",
    "            try:\n",
    "                unassigned_usages[lemma][\"usages\"].append({\n",
    "                        \"sentence\": sentence,\n",
    "                        \"target\": target,\n",
    "                        \"closest\": (sims[0], sims[1]),\n",
    "                    })\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    # print percentage of progress\n",
    "    if i % 100 == 0:\n",
    "        print(f\"{i/inst*100:.2f}% done\", end=\"\\r\")\n",
    "    i += 1\n",
    "\n",
    "# remove lemmas with no unassigned usages\n",
    "unassigned_usages = {k: v for k, v in unassigned_usages.items() if len(v[\"usages\"]) > 0}\n",
    "\n",
    "print(\"Number of unknown usages: \", len(unassigned_usages))\n",
    "with open(\"../data/results/swe_hist_sp_f3_100k_unassigned_prediction.json\", \"w\") as f:\n",
    "    json.dump(unassigned_usages, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/results/swe_hist_sp_f3_100k_unassigned_prediction.json\", \"r\") as f:\n",
    "    unassigned_usages = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_covered = []\n",
    "\n",
    "for lemma in dict_sense_id:\n",
    "    #complete_examples = [e for e in lemma[\"entries\"] if e[\"examples\"] != []]\n",
    "    complete_covered.append((lemma['key'], int( not (len([g for g in lemma[\"entries\"] if g[\"sense\"] != \"\"]) - len(lemma[\"entries\"])) == 0)))\n",
    "    if lemma['key'] == \"avsnitt\":\n",
    "        print(complete_covered[-1])\n",
    "        print(lemma)\n",
    "        print([g for g in lemma[\"entries\"] if g[\"sense\"] != \"\"])\n",
    "\n",
    "complete_covered = sorted(complete_covered, key=lambda x: x[1])\n",
    "print(complete_covered[1150:1155])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort unknown usages by percentage of completeness\n",
    "sorted_unassigned_usages = []\n",
    "for c in complete_covered:\n",
    "    if c[0] in unassigned_usages.keys():\n",
    "        sorted_unassigned_usages.append({\n",
    "            \"lemma\": c[0],\n",
    "            \"missing_synsets\": c[1],\n",
    "            \"unassigned_usages\": unassigned_usages[c[0]]\n",
    "        })\n",
    "display(sorted_unassigned_usages[6:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u in sorted_unassigned_usages:\n",
    "    # sort unassigend usages by similarity\n",
    "    u[\"unassigned_usages\"][\"usages\"] = sorted(u[\"unassigned_usages\"][\"usages\"], key=lambda x: x[\"closest\"][1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(sorted_unassigned_usages, f, indent=4, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
