{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build annotation data\n",
    "This notebook generates all data needed for the WSBest annotation on PhiTag:\n",
    "- `senses.tsv` all senses that appear in the sample\n",
    "- `usages.tsv` all usages that appear in the sample\n",
    "- `instances.tsv` all possible combinations of senses and usages\n",
    "\n",
    "The data is generated from the model predictions.\n",
    "\n",
    "### Usage\n",
    "Set the correct path to the model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from spacy.symbols import IS_PUNCT\n",
    "import json\n",
    "from nltk.corpus import wordnet as wn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_result_file = \"../data/outputs/annotation_phase_2/eng_combined_data.json\"\n",
    "#model_result_file = \"../data_sampling_2/data/results/100k_unassigned_prediction_sorted_reevaluated.json\"\n",
    "#model_result_file = \"../data_sampling_2/data/annotation_data/sp_f3_150k_unassigned_prediction_sorted_reevaluated[700].json\"\n",
    "#model_result_file = \"../data_sampling_2/data/annotation_data/combined_data.json\"\n",
    "#model_result_file = \"../data_sampling_2/data/annotation_data/swe_combined_data.json\"\n",
    "\n",
    "dictionary_file = \"../data/dictionaries/wordnet_sense_id.json\"\n",
    "#dictionary_file = \"../data_analysis/data/dictionaries/sw_dict_sense_id.json\"\n",
    "\n",
    "sub_dir = \"/ws_best_en/\"\n",
    "\n",
    "# Output\n",
    "instances_file = f\"../data/outputs/annotation_phase_2/{sub_dir}instances.tsv\"\n",
    "senses_file = f\"../data/outputs/annotation_phase_2/{sub_dir}senses.tsv\"\n",
    "usages_file = f\"../data/outputs/annotation_phase_2/{sub_dir}usages.tsv\"\n",
    "\n",
    "#instances_file = f\"../data_sampling_2/data/annotation_data/swe_{model_result_file.split('/')[-1].split('.')[0]}_instances.tsv\"\n",
    "#senses_file = f\"../data_sampling_2/data/annotation_data/swe_{model_result_file.split('/')[-1].split('.')[0]}_senses.tsv\"\n",
    "#usages_file = f\"../data_sampling_2/data/annotation_data/swe_{model_result_file.split('/')[-1].split('.')[0]}_usages.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'lemma': 'qualification',\n",
       "  'sentence': \"Slovan Bratislava's Champions League qualification tie against Faroe Islands side KI Klaksvik has been cancelled after a player from the Slovakian club tested positive for COVID19, European soccer's governing body UEFA said on Saturday.\",\n",
       "  'target': [37, 50],\n",
       "  'highest_similarity': 0.173049004,\n",
       "  'corpus_id': 'leipzig_eng_news'},\n",
       " {'lemma': 'qualification',\n",
       "  'sentence': 'Streichs team is currently eighth in the 18team Bundesliga, two points behind Hoffenheim in the last place for European qualification.',\n",
       "  'target': [120, 133],\n",
       "  'highest_similarity': 0.1888046389,\n",
       "  'corpus_id': 'leipzig_eng_news'},\n",
       " {'lemma': 'qualification',\n",
       "  'sentence': 'But they have fallen away badly, winning only five times in the league in 2020 and they are in real danger of missing out on a Champions League qualification spot.',\n",
       "  'target': [144, 157],\n",
       "  'highest_similarity': 0.1922517948,\n",
       "  'corpus_id': 'leipzig_eng_news'},\n",
       " {'lemma': 'qualification',\n",
       "  'sentence': 'The bid for Aberdeens first Scottish Cup since 1990 and European qualification are the primary concerns for McGeouch as well as getting back to playing regular football after a frustrating time on the sidelines at the Stadium of Light.',\n",
       "  'target': [65, 78],\n",
       "  'highest_similarity': 0.1944697268,\n",
       "  'corpus_id': 'leipzig_eng_news'},\n",
       " {'lemma': 'wage',\n",
       "  'sentence': 'Davis gained fame as a state legislator by waging a 2013 filibuster against an antiabortion bill, then lost a race for governor the following year.',\n",
       "  'target': [43, 47],\n",
       "  'highest_similarity': 0.1949448247,\n",
       "  'corpus_id': 'leipzig_eng_news'},\n",
       " {'lemma': 'qualification',\n",
       "  'sentence': 'Egbo, 47, led KF Tirana to their first league title in a decade back in July becoming the first African coach to lead a European team to a league title and qualification to the UEFA Champions League.',\n",
       "  'target': [156, 169],\n",
       "  'highest_similarity': 0.2220004493,\n",
       "  'corpus_id': 'leipzig_eng_news'},\n",
       " {'lemma': 'unused',\n",
       "  'sentence': 'During my voyage , affrighted by the dangers which surrounded me , and to which I was wholly unused , I heartily repented of my resolution but now , methinks , I have reason to rejoice at my perseverance .',\n",
       "  'target': [93, 99],\n",
       "  'highest_similarity': 0.2237192328,\n",
       "  'corpus_id': 'ccoha1'},\n",
       " {'lemma': 'qualification',\n",
       "  'sentence': 'The top two teams of each group secure direct qualification while six thirdplaced teams will compete in a separate competition to win the remaining four places.',\n",
       "  'target': [46, 59],\n",
       "  'highest_similarity': 0.2249246634,\n",
       "  'corpus_id': 'leipzig_eng_news'},\n",
       " {'lemma': 'trample',\n",
       "  'sentence': 'Trump also reversed Obamas policies that trampled on the religious freedom of prolife charities.',\n",
       "  'target': [41, 48],\n",
       "  'highest_similarity': 0.2269072648,\n",
       "  'corpus_id': 'leipzig_eng_news'},\n",
       " {'lemma': 'wage',\n",
       "  'sentence': 'This tragic failure, however, is an inevitable consequence of the national security establishments determination to ignore critical domestic problems for the sake of waging unending but profitable Global Wars on Terror and new Cold Wars.',\n",
       "  'target': [166, 170],\n",
       "  'highest_similarity': 0.2291323381,\n",
       "  'corpus_id': 'leipzig_eng_news'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(model_result_file, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "with open(dictionary_file, \"r\") as f:\n",
    "    dictionary = json.load(f)\n",
    "\n",
    "    global_corpus_id_modern = 0\n",
    "    global_corpus_id_historical = 0\n",
    "\n",
    "    instances = []\n",
    "    senses = []\n",
    "    usages = []\n",
    "\n",
    "    # sort usages by similarity score\n",
    "    data = sorted(data, key=lambda k: k['highest_similarity'])\n",
    "    display(data[0:10])\n",
    "\n",
    "    for usage in data:\n",
    "\n",
    "        for entry in dictionary:\n",
    "            if entry['key'] == usage['lemma']:\n",
    "                dictionary_entry = entry\n",
    "                break\n",
    "\n",
    "        # add senses\n",
    "\n",
    "        sense_ids = []\n",
    "\n",
    "        for sense in dictionary_entry['entries']:\n",
    "            if sense['sense'] != '':\n",
    "                gloss = sense['sense']\n",
    "                if gloss == '':\n",
    "                    gloss = sense['senseSecondary']\n",
    "                \n",
    "                sense_ids.append(sense['identifier'])\n",
    "                \n",
    "                senses.append({\n",
    "                    \"senseID\": sense['identifier'],\n",
    "                    \"definition\": gloss,\n",
    "                    \"lemma\": usage['lemma']\n",
    "                })  \n",
    "\n",
    "        # add usages\n",
    "        if usage['corpus_id'] == \"leipzig_swe_news\":\n",
    "            index = global_corpus_id_modern\n",
    "            global_corpus_id_modern += 1\n",
    "        else:\n",
    "            index = global_corpus_id_historical\n",
    "            global_corpus_id_historical += 1\n",
    "\n",
    "        target = f\"{usage['target'][0]}:{usage['target'][1]}\"\n",
    "\n",
    "        id = f\"combined_sample_{usage['corpus_id']}-{usage['lemma']}-{index}-{target}\"\n",
    "\n",
    "        usages.append({\n",
    "        \"dataID\": id,\n",
    "        \"context\": usage['sentence'],\n",
    "        \"indices_target_token\": target,\n",
    "        \"indices_target_sentence\": f\"0:{len(usage['sentence'])}\",\n",
    "        \"lemma\": usage['lemma']\n",
    "        })\n",
    "\n",
    "        if len(instances) > 1400:\n",
    "            break\n",
    "\n",
    "        # skip if more than 5 usages of that lemma are already in the dataset\n",
    "        if len([u for u in usages if u['lemma'] == usage['lemma']]) > 3:\n",
    "            #print(f\"Skipping {usage['lemma']}, already in dataset\")\n",
    "            continue        \n",
    "\n",
    "        for s in sense_ids:\n",
    "            instances.append({\n",
    "                \"instanceID\": f\"{id}-{s}\",\n",
    "                \"dataIDs\": f\"{id},{s}\",\n",
    "                \"label_set\": \"0,1\",\n",
    "                \"non_label\": \"-\"\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct target\n",
    "for u in usages:\n",
    "    target = list(map(int, u[\"indices_target_token\"].split(\":\")))\n",
    "    context = u[\"context\"]\n",
    "\n",
    "    #print(context[target[0]:target[1]])\n",
    "    if target[1] < len(context):\n",
    "        while context[target[1]].isalpha():\n",
    "            target[1] += 1\n",
    "            if target[1] == len(context):\n",
    "                break\n",
    "    \n",
    "    # print(context[target[0]:target[1]])\n",
    "\n",
    "    u[\"indices_target_token\"] = f\"{target[0]}:{target[1]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances: 495\n",
      "Number of senses: 960\n",
      "Number of usages: 424\n",
      "Number of unique lemmas: 115\n",
      "Number of usages after reduction: 247\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "print(f\"Number of instances: {len(instances)}\")\n",
    "print(f\"Number of senses: {len(senses)}\")\n",
    "print(f\"Number of usages: {len(usages)}\")\n",
    "\n",
    "print(f\"Number of unique lemmas: {len(set([u['lemma'] for u in usages]))}\")\n",
    "# reduce usages to maxium of 5 ranodm per lemma, less if there are less\n",
    "usages = pd.DataFrame(usages)\n",
    "usages = usages.groupby(\"lemma\").apply(lambda x: x.sample(min(len(x), 5))).reset_index(drop=True)\n",
    "print(f\"Number of usages after reduction: {len(usages)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instanceID</th>\n",
       "      <th>dataIDs</th>\n",
       "      <th>label_set</th>\n",
       "      <th>non_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>attend-combined_sample_ccoha1-272-46:52-attend...</td>\n",
       "      <td>combined_sample_ccoha1-attend-272-46:52,attend...</td>\n",
       "      <td>0,1</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>attend-combined_sample_ccoha1-272-46:52-attend...</td>\n",
       "      <td>combined_sample_ccoha1-attend-272-46:52,attend...</td>\n",
       "      <td>0,1</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>attend-combined_sample_ccoha1-272-46:52-attend...</td>\n",
       "      <td>combined_sample_ccoha1-attend-272-46:52,attend...</td>\n",
       "      <td>0,1</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>attend-combined_sample_ccoha1-272-46:52-attend...</td>\n",
       "      <td>combined_sample_ccoha1-attend-272-46:52,attend...</td>\n",
       "      <td>0,1</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>attend-combined_sample_ccoha1-368-38:44-attend...</td>\n",
       "      <td>combined_sample_ccoha1-attend-368-38:44,attend...</td>\n",
       "      <td>0,1</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>attend-combined_sample_ccoha1-368-38:44-attend...</td>\n",
       "      <td>combined_sample_ccoha1-attend-368-38:44,attend...</td>\n",
       "      <td>0,1</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>attend-combined_sample_ccoha1-368-38:44-attend...</td>\n",
       "      <td>combined_sample_ccoha1-attend-368-38:44,attend...</td>\n",
       "      <td>0,1</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>attend-combined_sample_ccoha1-368-38:44-attend...</td>\n",
       "      <td>combined_sample_ccoha1-attend-368-38:44,attend...</td>\n",
       "      <td>0,1</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>attend-combined_sample_ccoha1-373-62:68-attend...</td>\n",
       "      <td>combined_sample_ccoha1-attend-373-62:68,attend...</td>\n",
       "      <td>0,1</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>attend-combined_sample_ccoha1-373-62:68-attend...</td>\n",
       "      <td>combined_sample_ccoha1-attend-373-62:68,attend...</td>\n",
       "      <td>0,1</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            instanceID  \\\n",
       "296  attend-combined_sample_ccoha1-272-46:52-attend...   \n",
       "297  attend-combined_sample_ccoha1-272-46:52-attend...   \n",
       "298  attend-combined_sample_ccoha1-272-46:52-attend...   \n",
       "299  attend-combined_sample_ccoha1-272-46:52-attend...   \n",
       "418  attend-combined_sample_ccoha1-368-38:44-attend...   \n",
       "419  attend-combined_sample_ccoha1-368-38:44-attend...   \n",
       "420  attend-combined_sample_ccoha1-368-38:44-attend...   \n",
       "421  attend-combined_sample_ccoha1-368-38:44-attend...   \n",
       "423  attend-combined_sample_ccoha1-373-62:68-attend...   \n",
       "424  attend-combined_sample_ccoha1-373-62:68-attend...   \n",
       "\n",
       "                                               dataIDs label_set non_label  \n",
       "296  combined_sample_ccoha1-attend-272-46:52,attend...       0,1         -  \n",
       "297  combined_sample_ccoha1-attend-272-46:52,attend...       0,1         -  \n",
       "298  combined_sample_ccoha1-attend-272-46:52,attend...       0,1         -  \n",
       "299  combined_sample_ccoha1-attend-272-46:52,attend...       0,1         -  \n",
       "418  combined_sample_ccoha1-attend-368-38:44,attend...       0,1         -  \n",
       "419  combined_sample_ccoha1-attend-368-38:44,attend...       0,1         -  \n",
       "420  combined_sample_ccoha1-attend-368-38:44,attend...       0,1         -  \n",
       "421  combined_sample_ccoha1-attend-368-38:44,attend...       0,1         -  \n",
       "423  combined_sample_ccoha1-attend-373-62:68,attend...       0,1         -  \n",
       "424  combined_sample_ccoha1-attend-373-62:68,attend...       0,1         -  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# control for duplicates\n",
    "\n",
    "# change istanceIDs (split - and move [1] to front)\n",
    "instances = pd.DataFrame(instances)\n",
    "instances = instances.drop_duplicates(subset=['instanceID'])\n",
    "instances['instanceID'] = instances['instanceID'].apply(lambda x: x.split(\"-\")[1] + \"-\" + x.replace(x.split(\"-\")[1] + '-', \"\"))\n",
    "# alphabetically sort instanceIDs\n",
    "instances = instances.sort_values(by=['instanceID'])\n",
    "display(instances.head(10))\n",
    "\n",
    "\n",
    "instances = instances.drop_duplicates(subset=['instanceID'])\n",
    "instances.to_csv(instances_file, sep='\\t', index=False)\n",
    "\n",
    "\n",
    "senses = pd.DataFrame(senses)\n",
    "senses = senses.drop_duplicates(subset=['senseID'])\n",
    "senses.to_csv(senses_file, sep='\\t', index=False)\n",
    "\n",
    "usages = pd.DataFrame(usages)\n",
    "usages = usages.drop_duplicates(subset=['dataID'])\n",
    "usages.to_csv(usages_file, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances: 495\n",
      "Instances: 495\n"
     ]
    }
   ],
   "source": [
    "print(f\"Instances: {len(instances)}\")\n",
    "# remove duplicate instances\n",
    "instances = instances.drop_duplicates(subset=['instanceID'])\n",
    "print(f\"Instances: {len(instances)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
