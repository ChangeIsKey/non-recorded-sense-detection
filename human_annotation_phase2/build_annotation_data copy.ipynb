{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build annotation data\n",
    "This notebook generates all data needed for the WSBest annotation on PhiTag:\n",
    "- `senses.tsv` all senses that appear in the sample\n",
    "- `usages.tsv` all usages that appear in the sample\n",
    "- `instances.tsv` all possible combinations of senses and usages\n",
    "\n",
    "The data is generated from the model predictions.\n",
    "\n",
    "### Usage\n",
    "Set the correct path to the model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "from nltk.corpus import wordnet as wn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_result_file = \"../data/outputs/annotation_phase_2/eng_combined_data.json\"\n",
    "\n",
    "dictionary_file = \"../data/dictionaries/wordnet_sense_id.json\"\n",
    "#dictionary_file = \"../data_analysis/data/dictionaries/sw_dict_sense_id.json\"\n",
    "\n",
    "sub_dir = \"/ws_best_en/\"\n",
    "\n",
    "# Output\n",
    "instances_file = f\"../data/outputs/annotation_phase_2/{sub_dir}instances.tsv\"\n",
    "senses_file = f\"../data/outputs/annotation_phase_2/{sub_dir}senses.tsv\"\n",
    "usages_file = f\"../data/outputs/annotation_phase_2/{sub_dir}usages.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'lemma': 'qualification',\n",
       "  'sentence': \"Slovan Bratislava's Champions League qualification tie against Faroe Islands side KI Klaksvik has been cancelled after a player from the Slovakian club tested positive for COVID19, European soccer's governing body UEFA said on Saturday.\",\n",
       "  'target': [37, 50],\n",
       "  'highest_similarity': 0.173049004,\n",
       "  'corpus_id': 'leipzig_eng_news'},\n",
       " {'lemma': 'qualification',\n",
       "  'sentence': 'Streichs team is currently eighth in the 18team Bundesliga, two points behind Hoffenheim in the last place for European qualification.',\n",
       "  'target': [120, 133],\n",
       "  'highest_similarity': 0.1888046389,\n",
       "  'corpus_id': 'leipzig_eng_news'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(model_result_file, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "with open(dictionary_file, \"r\") as f:\n",
    "    dictionary = json.load(f)\n",
    "\n",
    "    global_corpus_id_modern = 0\n",
    "    global_corpus_id_historical = 0\n",
    "\n",
    "    instances = []\n",
    "    senses = []\n",
    "    usages = []\n",
    "\n",
    "    # sort usages by similarity score\n",
    "    data = sorted(data, key=lambda k: k['highest_similarity'])\n",
    "    display(data[:2])\n",
    "\n",
    "    for usage in data:\n",
    "\n",
    "        for entry in dictionary:\n",
    "            if entry['key'] == usage['lemma']:\n",
    "                dictionary_entry = entry\n",
    "                break\n",
    "\n",
    "        # add senses\n",
    "\n",
    "        sense_ids = []\n",
    "\n",
    "        for sense in dictionary_entry['entries']:\n",
    "            if sense['sense'] != '':\n",
    "                gloss = sense['sense']\n",
    "                if gloss == '':\n",
    "                    gloss = sense['senseSecondary']\n",
    "                \n",
    "                sense_ids.append(sense['identifier'])\n",
    "                \n",
    "                senses.append({\n",
    "                    \"senseID\": sense['identifier'],\n",
    "                    \"definition\": gloss,\n",
    "                    \"lemma\": usage['lemma']\n",
    "                })  \n",
    "\n",
    "        # add usages\n",
    "        if usage['corpus_id'] == \"leipzig_swe_news\":\n",
    "            index = global_corpus_id_modern\n",
    "            global_corpus_id_modern += 1\n",
    "        else:\n",
    "            index = global_corpus_id_historical\n",
    "            global_corpus_id_historical += 1\n",
    "\n",
    "        target = f\"{usage['target'][0]}:{usage['target'][1]}\"\n",
    "\n",
    "        id = f\"combined_sample_{usage['corpus_id']}-{usage['lemma']}-{index}-{target}\"\n",
    "\n",
    "        usages.append({\n",
    "        \"dataID\": id,\n",
    "        \"context\": usage['sentence'],\n",
    "        \"indices_target_token\": target,\n",
    "        \"indices_target_sentence\": f\"0:{len(usage['sentence'])}\",\n",
    "        \"lemma\": usage['lemma']\n",
    "        })\n",
    "\n",
    "        if len(instances) > 1400:\n",
    "            break\n",
    "\n",
    "        # skip if more than 5 usages of that lemma are already in the dataset\n",
    "        if len([u for u in usages if u['lemma'] == usage['lemma']]) > 3:\n",
    "            continue        \n",
    "\n",
    "        for s in sense_ids:\n",
    "            instances.append({\n",
    "                \"instanceID\": f\"{id}-{s}\",\n",
    "                \"dataIDs\": f\"{id},{s}\",\n",
    "                \"label_set\": \"0,1\",\n",
    "                \"non_label\": \"-\"\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct target\n",
    "for u in usages:\n",
    "    target = list(map(int, u[\"indices_target_token\"].split(\":\")))\n",
    "    context = u[\"context\"]\n",
    "\n",
    "    if target[1] < len(context):\n",
    "        while context[target[1]].isalpha():\n",
    "            target[1] += 1\n",
    "            if target[1] == len(context):\n",
    "                break\n",
    "    \n",
    "    u[\"indices_target_token\"] = f\"{target[0]}:{target[1]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances: 495\n",
      "Number of senses: 960\n",
      "Number of usages: 424\n",
      "Number of unique lemmas: 115\n",
      "Number of usages after reduction: 247\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "print(f\"Number of instances: {len(instances)}\")\n",
    "print(f\"Number of senses: {len(senses)}\")\n",
    "print(f\"Number of usages: {len(usages)}\")\n",
    "\n",
    "print(f\"Number of unique lemmas: {len(set([u['lemma'] for u in usages]))}\")\n",
    "# reduce usages to maxium of 5 ranodm per lemma, less if there are less\n",
    "usages = pd.DataFrame(usages)\n",
    "usages = usages.groupby(\"lemma\").apply(lambda x: x.sample(min(len(x), 5))).reset_index(drop=True)\n",
    "print(f\"Number of usages after reduction: {len(usages)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instanceID</th>\n",
       "      <th>dataIDs</th>\n",
       "      <th>label_set</th>\n",
       "      <th>non_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>attend-combined_sample_ccoha1-272-46:52-attend...</td>\n",
       "      <td>combined_sample_ccoha1-attend-272-46:52,attend...</td>\n",
       "      <td>0,1</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>attend-combined_sample_ccoha1-272-46:52-attend...</td>\n",
       "      <td>combined_sample_ccoha1-attend-272-46:52,attend...</td>\n",
       "      <td>0,1</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>attend-combined_sample_ccoha1-272-46:52-attend...</td>\n",
       "      <td>combined_sample_ccoha1-attend-272-46:52,attend...</td>\n",
       "      <td>0,1</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>attend-combined_sample_ccoha1-272-46:52-attend...</td>\n",
       "      <td>combined_sample_ccoha1-attend-272-46:52,attend...</td>\n",
       "      <td>0,1</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>attend-combined_sample_ccoha1-368-38:44-attend...</td>\n",
       "      <td>combined_sample_ccoha1-attend-368-38:44,attend...</td>\n",
       "      <td>0,1</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>attend-combined_sample_ccoha1-368-38:44-attend...</td>\n",
       "      <td>combined_sample_ccoha1-attend-368-38:44,attend...</td>\n",
       "      <td>0,1</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>attend-combined_sample_ccoha1-368-38:44-attend...</td>\n",
       "      <td>combined_sample_ccoha1-attend-368-38:44,attend...</td>\n",
       "      <td>0,1</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>attend-combined_sample_ccoha1-368-38:44-attend...</td>\n",
       "      <td>combined_sample_ccoha1-attend-368-38:44,attend...</td>\n",
       "      <td>0,1</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>attend-combined_sample_ccoha1-373-62:68-attend...</td>\n",
       "      <td>combined_sample_ccoha1-attend-373-62:68,attend...</td>\n",
       "      <td>0,1</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>attend-combined_sample_ccoha1-373-62:68-attend...</td>\n",
       "      <td>combined_sample_ccoha1-attend-373-62:68,attend...</td>\n",
       "      <td>0,1</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            instanceID  \\\n",
       "296  attend-combined_sample_ccoha1-272-46:52-attend...   \n",
       "297  attend-combined_sample_ccoha1-272-46:52-attend...   \n",
       "298  attend-combined_sample_ccoha1-272-46:52-attend...   \n",
       "299  attend-combined_sample_ccoha1-272-46:52-attend...   \n",
       "418  attend-combined_sample_ccoha1-368-38:44-attend...   \n",
       "419  attend-combined_sample_ccoha1-368-38:44-attend...   \n",
       "420  attend-combined_sample_ccoha1-368-38:44-attend...   \n",
       "421  attend-combined_sample_ccoha1-368-38:44-attend...   \n",
       "423  attend-combined_sample_ccoha1-373-62:68-attend...   \n",
       "424  attend-combined_sample_ccoha1-373-62:68-attend...   \n",
       "\n",
       "                                               dataIDs label_set non_label  \n",
       "296  combined_sample_ccoha1-attend-272-46:52,attend...       0,1         -  \n",
       "297  combined_sample_ccoha1-attend-272-46:52,attend...       0,1         -  \n",
       "298  combined_sample_ccoha1-attend-272-46:52,attend...       0,1         -  \n",
       "299  combined_sample_ccoha1-attend-272-46:52,attend...       0,1         -  \n",
       "418  combined_sample_ccoha1-attend-368-38:44,attend...       0,1         -  \n",
       "419  combined_sample_ccoha1-attend-368-38:44,attend...       0,1         -  \n",
       "420  combined_sample_ccoha1-attend-368-38:44,attend...       0,1         -  \n",
       "421  combined_sample_ccoha1-attend-368-38:44,attend...       0,1         -  \n",
       "423  combined_sample_ccoha1-attend-373-62:68,attend...       0,1         -  \n",
       "424  combined_sample_ccoha1-attend-373-62:68,attend...       0,1         -  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# control for duplicates\n",
    "\n",
    "# change istanceIDs (split - and move [1] to front)\n",
    "instances = pd.DataFrame(instances)\n",
    "instances = instances.drop_duplicates(subset=['instanceID'])\n",
    "instances['instanceID'] = instances['instanceID'].apply(lambda x: x.split(\"-\")[1] + \"-\" + x.replace(x.split(\"-\")[1] + '-', \"\"))\n",
    "# alphabetically sort instanceIDs\n",
    "instances = instances.sort_values(by=['instanceID'])\n",
    "display(instances.head(10))\n",
    "\n",
    "\n",
    "instances = instances.drop_duplicates(subset=['instanceID'])\n",
    "instances.to_csv(instances_file, sep='\\t', index=False)\n",
    "\n",
    "\n",
    "senses = pd.DataFrame(senses)\n",
    "senses = senses.drop_duplicates(subset=['senseID'])\n",
    "senses.to_csv(senses_file, sep='\\t', index=False)\n",
    "\n",
    "usages = pd.DataFrame(usages)\n",
    "usages = usages.drop_duplicates(subset=['dataID'])\n",
    "usages.to_csv(usages_file, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances: 495\n",
      "Instances without duplicates: 495\n"
     ]
    }
   ],
   "source": [
    "print(f\"Instances: {len(instances)}\")\n",
    "# remove duplicate instances\n",
    "instances = instances.drop_duplicates(subset=['instanceID'])\n",
    "print(f\"Instances without duplicates: {len(instances)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
